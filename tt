# OpenShift Enterprise Deployment Guide

## Table of Contents

- [Overview](#overview)
- [Audience and Prerequisites](#audience-and-prerequisites)
- [Quick Start](#quick-start)
- [Architecture Overview](#architecture-overview)
- [System Views](#system-views)
  - [C4 Model Architecture](#c4-model-architecture)
- [Deployment Procedures](#deployment-procedures)
  - [Infrastructure Setup](#infrastructure-setup)
  - [Management Cluster Deployment](#management-cluster-deployment)
  - [Hosted Cluster Provisioning](#hosted-cluster-provisioning)
- [Configuration Management](#configuration-management)
- [Operations and Maintenance](#operations-and-maintenance)
- [Troubleshooting](#troubleshooting)
- [Reference](#reference)

## Overview

This guide provides practical instructions for deploying and managing an enterprise-grade OpenShift Platform Plus environment across two independent data centers. OpenShift Platform Plus delivers a comprehensive, pre-integrated solution that includes OpenShift Container Platform with advanced management, GitOps, logging, monitoring, and security capabilities.

### OpenShift Platform Plus Capabilities

OpenShift Platform Plus provides a comprehensive, enterprise-ready container platform with integrated advanced capabilities:

#### Core Platform Features

- **OpenShift Container Platform**: Enterprise Kubernetes with advanced security, compliance, and lifecycle management
- **HyperShift**: Hosted control planes for efficient multi-cluster management
- **Advanced Cluster Management**: Centralized control of multiple OpenShift clusters
- **Multi-environment support**: Engineering, Development, Testing (NP1/NP2), Production, and Member Test environments

#### Integrated Application Services

- **GitOps**: ArgoCD for declarative, automated application deployments and lifecycle management

#### Observability & Monitoring

- **OpenShift Logging**: Loki-based centralized logging with multi-tenancy and retention policies
- **OpenShift Monitoring**: Prometheus-based metrics collection with Grafana visualization and Alertmanager
- **Distributed Tracing**: Jaeger integration for microservices observability
- **Alloy**: Unified telemetry collection for metrics, logs, and traces

#### Security & Compliance

- **Advanced Security**: Security Context Constraints, pod security standards, and integrated security scanning
- **Compliance Operator**: Automated CIS benchmark scanning and remediation
- **File Integrity Operator**: Continuous monitoring of file system integrity
- **Secrets Management**: HashiCorp Vault integration with Secrets Operator and cert-manager

#### Storage & Networking

- **Enterprise Storage**: NetApp Trident integration for persistent volumes and S3-compatible object storage
- **Advanced Networking**: MetalLB for load balancing, HAProxy/Nginx ingress controllers
- **External Load Balancing**: A10 Thunder and F5 BIG-IP integration for enterprise traffic management
- **Network Security**: Advanced network policies

#### Developer Experience

- **Developer Console**: Web-based IDE with integrated development tools
- **Topology View**: Visual application dependency mapping and management
- **Source-to-Image**: Automated container builds from source code
- **OperatorHub**: Centralized catalog of certified operators and applications

### Environment Distribution

| Environment | Data Center | Purpose                                                     |
| ----------- | ----------- | ----------------------------------------------------------- |
| Engineering | DC2         | Development prototyping and experiments                     |
| Development | DC2         | Active development and integration testing                  |
| NP1/NP2     | DC1         | Non-functional testing (performance, security, scalability) |
| Production  | DC1 + DC2   | Live production workloads with HA/DR                        |
| Member Test | DC1 + DC2   | Member-specific testing and validation                      |

## Audience and Prerequisites

### Target Audience

This guide is designed for:

- **Platform Engineers**: Responsible for deploying and maintaining OpenShift infrastructure
- **DevOps Engineers**: Managing CI/CD pipelines and application deployments
- **System Administrators**: Handling day-to-day operations and troubleshooting
- **Security Officers**: Implementing and maintaining security controls
- **Application Developers**: Understanding platform capabilities for application design

### Prerequisites

#### Infrastructure Requirements

- **Physical Servers**: Minimum 3 control plane nodes and 3 worker nodes per cluster
- **Network**: 10GbE networking with VLAN segregation
- **Storage**: Shared storage (NFS, Ceph, or enterprise SAN) with minimum 10TB capacity
- **Load Balancers**: MetalLB for in-cluster load balancing, A10 and F5 for external traffic management

#### OpenShift Platform Plus Subscription

**Required Subscription**: Red Hat OpenShift Platform Plus provides access to all integrated components and enterprise support.

**Included Software Components**:

- **OpenShift Container Platform**: Core platform with all enterprise features
- **OpenShift GitOps (ArgoCD)**: Integrated GitOps deployment and management
- **OpenShift Logging**: Loki-based centralized logging platform
- **OpenShift Monitoring**: Prometheus-based monitoring with Grafana
- **Compliance Operator**: Automated compliance scanning and remediation
- **File Integrity Operator**: File system integrity monitoring
- **Red Hat Enterprise Linux CoreOS (RHCOS)**: Optimized container OS for nodes

**Integrated Third-Party Components**:

- **HashiCorp Vault**: Enterprise secrets management (requires separate license)
- **cert-manager**: Certificate lifecycle management
- **NetApp Trident**: Enterprise storage orchestration
- **MetalLB**: Load balancing for bare metal environments
- **External Load Balancers**: A10 Thunder and F5 BIG-IP integration support

**Management Tools**:

- **Red Hat Ansible Automation Platform**: Configuration management and automation
- **Red Hat Advanced Cluster Management**: Multi-cluster management and governance

#### Knowledge Prerequisites

- Kubernetes concepts and OpenShift fundamentals
- Linux system administration
- Network infrastructure and security
- Certificate management and PKI concepts
- Infrastructure as Code (Terraform for Vault, Ansible for machine configuration)

#### Access Requirements

- Red Hat Customer Portal access for OCP downloads
- Administrative access to physical infrastructure
- DNS management capabilities
- Certificate authority access (if using external CA)

### Required Tools

```bash
# Core OpenShift CLI tools
oc version # OpenShift CLI
kubectl version # Kubernetes CLI

# Infrastructure provisioning
terraform version # Vault provisioning
ansible --version # Machine configuration

# Certificate management
vault version # HashiCorp Vault CLI
```

## Quick Start

For experienced users who want to deploy quickly:

### 1. Prepare Infrastructure

```bash
# Clone deployment repository
git clone https://github.com/your-org/openshift-deployment.git
cd openshift-deployment

# Initialize Terraform
terraform init

# Plan deployment
terraform plan -var-file=dc1.tfvars
```

### 2. Deploy Management Cluster

```bash
# Deploy management cluster in DC1
ansible-playbook playbooks/deploy-management-cluster.yml \
  -i inventory/dc1 \
  --extra-vars "cluster_name=mgmt-dc1"
```

### 3. Provision Hosted Clusters

```bash
# Create production hosted cluster
oc process -f templates/hosted-cluster.yml \
  -p CLUSTER_NAME=production \
  -p NODE_COUNT=6 \
  -p ENVIRONMENT=prod | oc apply -f -
```

### 4. Verify Deployment

```bash
# Check cluster status
oc get clusterversion
oc get nodes
oc get co # Check operator status
```

> **Note**: This quick start assumes you have all prerequisites met. For detailed procedures, see the [Deployment Procedures](#deployment-procedures) section.

## Architecture Overview

### Core Architecture

The deployment uses a HyperShift architecture with dedicated management clusters hosting control planes for multiple hosted clusters. This design provides:

- **Resource Efficiency**: Control planes run as pods on management clusters
- **Operational Simplicity**: Centralized management of multiple environments
- **Isolation**: Each hosted cluster is fully isolated with its own control plane
- **Scalability**: Easy provisioning of new environments without additional infrastructure

### OpenShift Platform Plus Component Architecture

| Component Category       | Included Components                                                   | Technology Stack                                    | Purpose                                                      |
| ------------------------ | --------------------------------------------------------------------- | --------------------------------------------------- | ------------------------------------------------------------ |
| **Core Platform**        | OpenShift Container Platform, HyperShift, Advanced Cluster Management | OpenShift 4.12+                                     | Enterprise Kubernetes platform with hosted control planes    |
| **Application Services** | GitOps                                                                | ArgoCD                                              | Declarative application deployments and lifecycle management |
| **Observability**        | Logging, Monitoring, Distributed Tracing                              | Loki, Prometheus, Grafana, Jaeger, Alloy            | Comprehensive observability and monitoring stack             |
| **Security**             | Compliance, File Integrity, Secrets Management                        | Compliance Operator, File Integrity Operator, Vault | Enterprise security and compliance automation                |
| **Storage & Networking** | Enterprise Storage, Load Balancing, Ingress                           | NetApp Trident, MetalLB, HAProxy, Nginx, A10, F5    | Production-ready storage and networking infrastructure       |
| **Developer Tools**      | Developer Console, Topology, Source-to-Image, OperatorHub             | Web Console, IDE, Build Tools                       | Integrated developer experience and tooling                  |

### OpenShift Platform Plus Security Architecture

OpenShift Platform Plus provides enterprise-grade security capabilities with integrated compliance and governance:

#### Platform Security Features

- **Security Context Constraints (SCCs)**: Granular pod security policies and privilege management
- **Pod Security Standards**: Kubernetes-native security standards enforcement
- **Network Policies**: Advanced network segmentation and traffic control
- **Compliance Operator**: Automated CIS benchmark scanning and remediation workflows
- **File Integrity Operator**: Continuous monitoring of file system integrity with AIDE

#### Identity & Access Management

- **Integrated Authentication**: OAuth 2.0, LDAP/AD, SAML, and OIDC support
- **Role-Based Access Control (RBAC)**: Fine-grained permission management
- **Multi-tenancy**: Secure namespace isolation and resource quotas
- **Audit Logging**: Comprehensive security event logging and monitoring

#### Secrets & Certificate Management

- **HashiCorp Vault Integration**: Enterprise secrets management with dynamic credentials
- **Automated Certificate Management**: cert-manager integration for certificate lifecycle
- **HSM Support**: Hardware Security Module integration for cryptographic operations
- **Secret Rotation**: Automated secret rotation and lease management

#### Compliance & Governance

- **Automated Compliance Scanning**: Continuous compliance monitoring and reporting
- **Security Policy Enforcement**: Automated remediation of security policy violations
- **Vulnerability Management**: Integrated security scanning and patch management
- **Regulatory Compliance**: Support for industry standards (PCI DSS, HIPAA, SOX, etc.)

## System Views

This section provides detailed visual representations of key system processes, resilience patterns, and scalability mechanisms.

### Sequence Diagrams

#### Application Deployment via ArgoCD

```mermaid
sequenceDiagram
    participant Dev as Developer
    participant Git as Git Repository
    participant ArgoCD as ArgoCD
    participant K8s as Kubernetes API
    participant App as Application

    Dev->>Git: Push application manifests
    ArgoCD->>Git: Poll for changes
    ArgoCD->>ArgoCD: Detect manifest changes
    ArgoCD->>K8s: Apply manifests via kubectl
    K8s->>K8s: Create/Update resources
    K8s->>App: Deploy application pods
    ArgoCD->>ArgoCD: Sync status check
    ArgoCD->>Dev: Report deployment status
```

#### Certificate Lifecycle Management

```mermaid
sequenceDiagram
    participant App as Application
    participant CertMgr as cert-manager
    participant Vault as HashiCorp Vault
    participant CA as Certificate Authority
    participant LB as Load Balancer

    App->>CertMgr: Request certificate
    CertMgr->>Vault: Check certificate status
    Vault->>CA: Request new certificate
    CA->>Vault: Issue certificate
    Vault->>CertMgr: Provide certificate
    CertMgr->>K8s: Create TLS secret
    K8s->>LB: Update SSL configuration
    LB->>App: Serve HTTPS traffic
```

#### Secret Retrieval Process

```mermaid
sequenceDiagram
    participant Pod as Application Pod
    participant K8s as Kubernetes
    participant VaultOp as Vault Secrets Operator
    participant Vault as HashiCorp Vault
    participant VSO as Vault Secrets

    Pod->>K8s: Request secret
    K8s->>VaultOp: Secret access request
    VaultOp->>Vault: Authenticate & authorize
    Vault->>VaultOp: Validate permissions
    VaultOp->>VSO: Retrieve secret data
    VSO->>VaultOp: Return encrypted secret
    VaultOp->>K8s: Inject secret as K8s secret
    K8s->>Pod: Mount secret volume
```

#### Cluster Scaling Operations

```mermaid
sequenceDiagram
    participant HPA as HorizontalPodAutoscaler
    participant Metrics as Metrics Server
    participant K8s as Kubernetes API
    participant Deploy as Deployment
    participant Nodes as Worker Nodes

    loop Monitoring
        Metrics->>HPA: Collect resource metrics
        HPA->>HPA: Evaluate scaling thresholds
    end

    alt Scale Up Required
        HPA->>K8s: Request scale up
        K8s->>Deploy: Increase replica count
        Deploy->>K8s: Schedule new pods
        K8s->>Nodes: Deploy pods on available nodes
    end

    alt Scale Down Required
        HPA->>K8s: Request scale down
        K8s->>Deploy: Decrease replica count
        Deploy->>K8s: Terminate excess pods
    end
```

### Resilience Views

#### Multi-Data Center Failover

```mermaid
graph TD
    subgraph "Data Center 1 (Primary)"
        DC1_LB[A10 Load Balancer]
        DC1_OCP[OpenShift Cluster]
        DC1_Storage[NetApp Storage]
        DC1_Vault[HashiCorp Vault]

        DC1_LB --> DC1_OCP
        DC1_OCP --> DC1_Storage
        DC1_OCP --> DC1_Vault
    end

    subgraph "Data Center 2 (Secondary)"
        DC2_LB[F5 Load Balancer]
        DC2_OCP[OpenShift Cluster]
        DC2_Storage[NetApp Storage]
        DC2_Vault[HashiCorp Vault]

        DC2_LB --> DC2_OCP
        DC2_OCP --> DC2_Storage
        DC2_OCP --> DC2_Vault
    end

    subgraph "Global Load Balancing"
        GSLB[Global Server Load Balancing]
    end

    DC1_LB -.-> GSLB
    DC2_LB -.-> GSLB

    subgraph "Disaster Recovery"
        DR[Automated Failover Scripts]
        Backup[Cross-DC Backups]
    end

    DC1_OCP -.-> Backup
    DC2_OCP -.-> Backup
    Backup -.-> DR
```

#### Component Redundancy Architecture

```mermaid
graph TD
    subgraph "Load Balancing Layer"
        A10_Primary[A10 Thunder<br/>Primary]
        A10_Backup[A10 Thunder<br/>Backup]
        F5_Primary[F5 BIG-IP<br/>Primary]
        F5_Backup[F5 BIG-IP<br/>Backup]
    end

    subgraph "Control Plane Layer"
        CP1[Control Plane 1<br/>Management Cluster]
        CP2[Control Plane 2<br/>Hosted Clusters]
        CP3[Control Plane 3<br/>Hosted Clusters]
    end

    subgraph "Data Layer"
        ETCD1[etcd Node 1]
        ETCD2[etcd Node 2]
        ETCD3[etcd Node 3]
        ETCD4[etcd Node 4]
        ETCD5[etcd Node 5]
    end

    subgraph "Storage Layer"
        Trident1[NetApp Trident<br/>Primary Array]
        Trident2[NetApp Trident<br/>Secondary Array]
        S3_1[S3 Compatible<br/>Primary]
        S3_2[S3 Compatible<br/>Secondary]
    end

    A10_Primary --> A10_Backup
    F5_Primary --> F5_Backup

    CP1 --> CP2
    CP1 --> CP3

    CP1 --> ETCD1
    CP1 --> ETCD2
    CP1 --> ETCD3
    CP2 --> ETCD3
    CP2 --> ETCD4
    CP3 --> ETCD4
    CP3 --> ETCD5

    CP1 --> Trident1
    CP2 --> Trident1
    CP3 --> Trident2

    Trident1 --> S3_1
    Trident2 --> S3_2
```

### Scalability Views

#### Horizontal Application Scaling

```mermaid
graph LR
    subgraph "Application Layer"
        HPA[Horizontal Pod Autoscaler]
        Deploy[Deployment<br/>replicas: 3-10]
        Pods[Pod 1<br/>Pod 2<br/>Pod 3<br/>...Pod N]
    end

    subgraph "Metrics Collection"
        Prometheus[Prometheus]
        Alloy[Alloy<br/>Metrics Collection]
        Mimir[Mimir<br/>Long-term Storage]
    end

    subgraph "Load Distribution"
        MetalLB[MetalLB<br/>Service Load Balancing]
        Ingress[HAProxy/Nginx<br/>Ingress Controllers]
        ExtLB[A10/F5<br/>External Load Balancers]
    end

    Prometheus --> HPA
    Alloy --> Prometheus
    Prometheus --> Mimir

    HPA --> Deploy
    Deploy --> Pods

    Pods --> MetalLB
    MetalLB --> Ingress
    Ingress --> ExtLB
```

#### Cluster Autoscaling Architecture

```mermaid
graph TD
    subgraph "Workload Monitoring"
        Metrics[Metrics Server<br/>Resource Usage]
        Prometheus[Prometheus<br/>Custom Metrics]
        HPA[HPA<br/>Pod Scaling]
        VPA[VPA<br/>Vertical Scaling]
    end

    subgraph "Cluster Autoscaling"
        CA[Cluster Autoscaler]
        MachineSets[Machine Sets<br/>Node Groups]
        Nodes[Worker Nodes]
    end

    subgraph "Infrastructure Scaling"
        Terraform[Terraform<br/>Infrastructure as Code]
        Ansible[Ansible<br/>Configuration]
        NetApp[NetApp Trident<br/>Storage Scaling]
    end

    Metrics --> CA
    Prometheus --> CA
    HPA --> CA
    VPA --> CA

    CA --> MachineSets
    MachineSets --> Nodes

    CA --> Terraform
    Terraform --> Ansible
    Ansible --> NetApp
```

#### Storage Scaling Mechanisms

```mermaid
graph TD
    subgraph "Dynamic Provisioning"
        PVC1[PVC Request<br/>50GB]
        PVC2[PVC Request<br/>100GB]
        StorageClass[Storage Class<br/>trident-nas]
    end

    subgraph "NetApp Trident"
        Trident[Trident CSI Driver]
        Backend[ONTAP Backend<br/>SVM Configuration]
        Volumes[FlexVol/FlexGroup<br/>Dynamic Volumes]
    end

    subgraph "Object Storage"
        S3PVC[S3 PVC Request]
        S3Backend[S3 Compatible<br/>Backend]
        Buckets[S3 Buckets<br/>Auto-scaling]
    end

    PVC1 --> StorageClass
    PVC2 --> StorageClass
    StorageClass --> Trident
    Trident --> Backend
    Backend --> Volumes

    S3PVC --> Trident
    Trident --> S3Backend
    S3Backend --> Buckets
```

### Disaster Recovery Sequence

````mermaid
sequenceDiagram
    participant Monitor as Monitoring System
    participant Alert as Alert Manager
    participant DR as DR Coordinator
    participant Backup as Backup System
    participant DC1 as Data Center 1
    participant DC2 as Data Center 2
    participant DNS as Global DNS

    Monitor->>Monitor: Detect failure
    Monitor->>Alert: Send alert
    Alert->>DR: Trigger DR process

    DR->>Backup: Verify backup integrity
    Backup->>DR: Confirm backup availability

    DR->>DC2: Initiate failover
    DC2->>DC2: Start secondary systems
    DC2->>DNS: Update DNS records
    DNS->>DNS: Point traffic to DC2

    DR->>DC1: Attempt recovery
    alt Recovery Successful
        DC1->>DR: Recovery complete
        DR->>DNS: Failback to DC1
    else Recovery Failed
        DR->>DC2: Promote to primary
        DR->>Alert: Send recovery notification
        end
    ```

    ### C4 Model Architecture

    Following the C4 model for software architecture visualization, this section provides detailed views of the system at different levels of abstraction.

    #### C1 - System Context (Enterprise Overview)

    The C1 diagram provides a comprehensive enterprise-level view of the OpenShift deployment, illustrating how the platform integrates within the broader organizational ecosystem. This view emphasizes the relationships between business stakeholders, operational teams, infrastructure components, and external systems.

    ##### Enterprise Architecture Overview

    The system operates within a complex enterprise environment with multiple stakeholder groups, each with specific roles and interaction patterns. The architecture supports a multi-environment strategy across two geographically distributed data centers, providing resilience, scalability, and operational flexibility.

    ##### Detailed System Context Diagram

    ```mermaid
    graph TB
        %% External Stakeholder Groups
        subgraph "Business Stakeholders"
            subgraph "Application Teams"
                AppDevs[Application Developers<br/>• Frontend/Backend Dev<br/>• Mobile Developers<br/>• API Developers<br/>• QA Engineers]
                ProductOwners[Product Owners<br/>• Feature Requests<br/>• Release Planning<br/>• User Story Management<br/>• Business Requirements]
                BusinessAnalysts[Business Analysts<br/>• Requirements Analysis<br/>• Process Modeling<br/>• User Experience<br/>• Data Analytics]
            end

            subgraph "End Users"
                InternalUsers[Internal Users<br/>• Employees<br/>• Departmental Apps<br/>• Internal Portals<br/>• Collaboration Tools]
                ExternalUsers[External Customers<br/>• Customer Portals<br/>• Partner Access<br/>• Public APIs<br/>• E-commerce]
                Partners[B2B Partners<br/>• Supplier Portals<br/>• Integration APIs<br/>• Data Exchange<br/>• Joint Services]
            end
        end

        subgraph "Operations Teams"
            subgraph "Platform Operations"
                PlatformEng[Platform Engineers<br/>• Infrastructure Design<br/>• Deployment Automation<br/>• Performance Tuning<br/>• Capacity Planning]
                DevOpsEng[DevOps Engineers<br/>• GitOps Workflows<br/>• Monitoring Setup<br/>• Incident Response]
                SecOps[Security Operations<br/>• Security Monitoring<br/>• Compliance Auditing<br/>• Threat Detection<br/>• Access Management]
            end

            subgraph "Infrastructure Operations"
                NetOps[Network Operations<br/>• Load Balancer Config<br/>• Network Security<br/>• DNS Management<br/>• Traffic Engineering]
                StorageOps[Storage Operations<br/>• NetApp Administration<br/>• Capacity Management<br/>• Backup Operations<br/>• Performance Monitoring]
                SysOps[Systems Operations<br/>• Server Provisioning<br/>• OS Management<br/>• Hardware Maintenance<br/>• Capacity Planning]
                DBAdmins[Database Administrators<br/>• Schema Management<br/>• Performance Tuning<br/>• Backup/Recovery<br/>• High Availability]
            end

            subgraph "Support Teams"
                AppSupport[Application Support<br/>• Incident Management<br/>• Problem Resolution<br/>• User Assistance<br/>• Knowledge Base]
                InfraSupport[Infrastructure Support<br/>• System Monitoring<br/>• Alert Response<br/>• Root Cause Analysis<br/>• Change Management]
            end
        end

        subgraph "Enterprise Infrastructure"
            subgraph "Data Center 1 (Primary)"
                DC1_Physical[Physical Infrastructure<br/>• Blade Servers<br/>• Network Switches<br/>• Storage Arrays<br/>• Power/Cooling]
                DC1_Network[Network Layer<br/>• Core Switching<br/>• Access Layer<br/>• VLAN Segmentation<br/>• QoS Policies]
                DC1_Security[Security Layer<br/>• Firewalls<br/>• IDS/IPS<br/>• Access Control<br/>• Encryption]
                DC1_LoadBal[A10 Thunder Load Balancers<br/>• SSL Termination<br/>• Health Monitoring<br/>• Traffic Distribution<br/>• DDoS Protection]
            end

            subgraph "Data Center 2 (Secondary)"
                DC2_Physical[Physical Infrastructure<br/>• Blade Servers<br/>• Network Switches<br/>• Storage Arrays<br/>• Power/Cooling]
                DC2_Network[Network Layer<br/>• Core Switching<br/>• Access Layer<br/>• VLAN Segmentation<br/>• QoS Policies]
                DC2_Security[Security Layer<br/>• Firewalls<br/>• IDS/IPS<br/>• Access Control<br/>• Encryption]
                DC2_LoadBal[F5 BIG-IP Load Balancers<br/>• Advanced Traffic Mgmt<br/>• Application Security<br/>• SSL Orchestration<br/>• API Gateway]
            end

            subgraph "Cross-DC Infrastructure"
                Interconnect[Layer 3 Interconnect<br/>• BGP Routing<br/>• VPN Tunnels<br/>• Direct Connect<br/>• Latency <5ms]
                GlobalDNS[Global DNS<br/>• Geo-DNS<br/>• Load Balancing<br/>• Failover Automation<br/>• Health Checks]
                GlobalLB[Global Load Balancing<br/>• Traffic Steering<br/>• Performance Routing<br/>• Disaster Recovery<br/>• Auto-scaling]
            end
        end

        subgraph "OpenShift Platform Ecosystem"
            subgraph "Management Clusters"
                Mgmt_DC1[Management Cluster DC1<br/>• 3 Control Plane Nodes<br/>• 6+ Worker Nodes<br/>• HyperShift Operator<br/>• Shared Services<br/>• ArgoCD Server<br/>• Observability Stack]
                Mgmt_DC2[Management Cluster DC2<br/>• 3 Control Plane Nodes<br/>• 6+ Worker Nodes<br/>• HyperShift Operator<br/>• Shared Services<br/>• ArgoCD Server<br/>• Observability Stack]
            end

            subgraph "Hosted Control Planes (DC1)"
                HCP_Prod_DC1[Production DC1 Control Plane<br/>• API Server Pod<br/>• etcd Pod<br/>• Controller Pods<br/>• Scheduler Pod<br/>• Isolated Namespace]
                HCP_NP1[NP1 Control Plane<br/>• API Server Pod<br/>• etcd Pod<br/>• Controller Pods<br/>• Scheduler Pod<br/>• Isolated Namespace]
                HCP_NP2[NP2 Control Plane<br/>• API Server Pod<br/>• etcd Pod<br/>• Controller Pods<br/>• Scheduler Pod<br/>• Isolated Namespace]
                HCP_Member_DC1[Member Test DC1 Control Plane<br/>• API Server Pod<br/>• etcd Pod<br/>• Controller Pods<br/>• Scheduler Pod<br/>• Isolated Namespace]
            end

            subgraph "Hosted Control Planes (DC2)"
                HCP_Prod_DC2[Production DC2 Control Plane<br/>• API Server Pod<br/>• etcd Pod<br/>• Controller Pods<br/>• Scheduler Pod<br/>• Isolated Namespace]
                HCP_Eng[Engineering Control Plane<br/>• API Server Pod<br/>• etcd Pod<br/>• Controller Pods<br/>• Scheduler Pod<br/>• Isolated Namespace]
                HCP_Dev[Development Control Plane<br/>• API Server Pod<br/>• etcd Pod<br/>• Controller Pods<br/>• Scheduler Pod<br/>• Isolated Namespace]
                HCP_Member_DC2[Member Test DC2 Control Plane<br/>• API Server Pod<br/>• etcd Pod<br/>• Controller Pods<br/>• Scheduler Pod<br/>• Isolated Namespace]
            end

            subgraph "Hosted Clusters (DC1)"
                Prod_DC1_Cluster[Production DC1 Environment<br/>• 6-12+ Worker Nodes<br/>• Live Applications<br/>• High Availability<br/>• Multi-tenancy<br/>• 99.9% SLA]
                NP1_Cluster[NP1 Environment<br/>• 3-6 Worker Nodes<br/>• Performance Testing<br/>• Load Testing Tools<br/>• JMeter, K6<br/>• Monitoring Integration]
                NP2_Cluster[NP2 Environment<br/>• 3-6 Worker Nodes<br/>• Security Testing<br/>• OWASP ZAP, Nessus<br/>• Compliance Scanning<br/>• Vulnerability Assessment]
                Member_DC1_Cluster[Member Test DC1<br/>• 3-6 Worker Nodes<br/>• Member Validation<br/>• Integration Testing<br/>• UAT Environments<br/>• Cross-DC Testing]
            end

            subgraph "Hosted Clusters (DC2)"
                Prod_DC2_Cluster[Production DC2 Environment<br/>• 6-12+ Worker Nodes<br/>• Disaster Recovery<br/>• Failover Target<br/>• Read Replicas<br/>• 99.9% SLA]
                Eng_Cluster[Engineering Environment<br/>• 3-6 Worker Nodes<br/>• Prototyping<br/>• Experimental Features<br/>• Development Tools<br/>• Innovation Sandbox]
                Dev_Cluster[Development Environment<br/>• 3-6 Worker Nodes<br/>• Integration Testing<br/>• CI/CD Validation<br/>• Code Quality Gates<br/>• Automated Testing]
                Member_DC2_Cluster[Member Test DC2<br/>• 3-6 Worker Nodes<br/>• Member Validation<br/>• Cross-DC Testing<br/>• Performance Validation<br/>• Multi-region Testing]
            end

            subgraph "Shared Services Layer"
                Vault_Cluster[HashiCorp Vault Enterprise<br/>• Secrets Management<br/>• Certificate Authority<br/>• Multi-DC Replication<br/>• HSM Integration<br/>• Audit Logging<br/>• Dynamic Secrets]
                Observability_Cluster[Grafana + Loki + Mimir + Alloy<br/>• Metrics Collection<br/>• Log Aggregation<br/>• Long-term Storage<br/>• Alerting Engine<br/>• Distributed Tracing<br/>• Custom Dashboards]
                ArgoCD_Cluster[ArgoCD Enterprise<br/>• GitOps Deployments<br/>• Application Sync<br/>• Multi-environment<br/>• Rollback Automation<br/>• Access Control<br/>• SSO Integration]
                Cert_Manager_Cluster[cert-manager + Vault<br/>• Certificate Issuance<br/>• Automated Renewal<br/>• ACME Integration<br/>• Enterprise CA<br/>• Certificate Monitoring]
            end
        end

        subgraph "Integration & Middleware"
            subgraph "API Management"
                API_Gateway[API Gateway Layer<br/>• Kong/Apigee Integration<br/>• Rate Limiting<br/>• API Analytics<br/>• Developer Portal]
            end

            subgraph "Integration Platforms"
                ESB[Enterprise Service Bus<br/>• Message Routing<br/>• Protocol Translation<br/>• Event Streaming<br/>• Workflow Orchestration]
                iPaaS[Integration Platform as a Service<br/>• Cloud Connectors<br/>• Data Mapping<br/>• Workflow Automation<br/>• API Management]
            end

            subgraph "Data Services"
                Database_Cluster[Database Services<br/>• PostgreSQL Clusters<br/>• MongoDB Replica Sets<br/>• Redis Clusters<br/>• Elasticsearch]
                Cache_Cluster[Caching Layer<br/>• Redis Cache<br/>• Memcached<br/>• CDN Integration<br/>• Edge Computing]
                Message_Queue[Message Queue<br/>• Kafka Clusters<br/>• RabbitMQ<br/>• Event Streaming<br/>• Message Persistence]
            end
        end

        subgraph "External Systems Integration"
            subgraph "Cloud Services"
                AWS_Services[AWS Integration<br/>• S3 Storage<br/>• RDS Databases<br/>• Lambda Functions<br/>• CloudFormation]
                Azure_Services[Azure Integration<br/>• Blob Storage<br/>• SQL Database<br/>• Functions<br/>• ARM Templates]
                GCP_Services[GCP Integration<br/>• Cloud Storage<br/>• Cloud SQL<br/>• Cloud Functions<br/>• Deployment Manager]
            end

            subgraph "Third-Party Systems"
                ERP_Systems[ERP Systems<br/>• SAP Integration<br/>• Oracle EBS<br/>• Microsoft Dynamics<br/>• API Connectors]
                CRM_Systems[CRM Systems<br/>• Salesforce<br/>• Microsoft Dynamics<br/>• HubSpot<br/>• API Integration]
                Legacy_Systems[Legacy Applications<br/>• Mainframe Integration<br/>• AS400 Systems<br/>• Custom Applications<br/>• ESB Mediation]
            end

            subgraph "Monitoring & Analytics"
                SIEM_Platform[SIEM Platform<br/>• Splunk Enterprise<br/>• IBM QRadar<br/>• LogRhythm<br/>• Security Analytics]
                ITSM_Platform[ITSM Platform<br/>• ServiceNow<br/>• BMC Remedy<br/>• Jira Service Desk<br/>• Incident Management]
                Analytics_Platform[Analytics Platform<br/>• Tableau<br/>• Power BI<br/>• Custom Dashboards<br/>• Business Intelligence]
            end
        end

        %% Business stakeholder interactions
        AppDevs --> Mgmt_DC1
        AppDevs --> Mgmt_DC2
        AppDevs --> ArgoCD_Cluster
        ProductOwners --> AppDevs
        BusinessAnalysts --> ProductOwners

        InternalUsers --> DC1_LoadBal
        InternalUsers --> DC2_LoadBal
        ExternalUsers --> GlobalLB
        Partners --> API_Gateway

        %% Operations team interactions
        PlatformEng --> Mgmt_DC1
        PlatformEng --> Mgmt_DC2
        PlatformEng --> Vault_Cluster
        DevOpsEng --> ArgoCD_Cluster
        DevOpsEng --> Observability_Cluster
        SecOps --> Vault_Cluster
        SecOps --> SIEM_Platform

        NetOps --> DC1_LoadBal
        NetOps --> DC2_LoadBal
        NetOps --> GlobalDNS
        StorageOps --> DC1_Physical
        StorageOps --> DC2_Physical
        SysOps --> Mgmt_DC1
        SysOps --> Mgmt_DC2
        DBAdmins --> Database_Cluster

        AppSupport --> ITSM_Platform
        InfraSupport --> Observability_Cluster

        %% Infrastructure connectivity
        DC1_LoadBal --> Prod_DC1_Cluster
        DC1_LoadBal --> NP1_Cluster
        DC1_LoadBal --> NP2_Cluster
        DC1_LoadBal --> Member_DC1_Cluster

        DC2_LoadBal --> Prod_DC2_Cluster
        DC2_LoadBal --> Eng_Cluster
        DC2_LoadBal --> Dev_Cluster
        DC2_LoadBal --> Member_DC2_Cluster

        GlobalLB --> DC1_LoadBal
        GlobalLB --> DC2_LoadBal
        GlobalDNS --> GlobalLB

        %% Management cluster to hosted control planes
        Mgmt_DC1 --> HCP_Prod_DC1
        Mgmt_DC1 --> HCP_NP1
        Mgmt_DC1 --> HCP_NP2
        Mgmt_DC1 --> HCP_Member_DC1

        Mgmt_DC2 --> HCP_Prod_DC2
        Mgmt_DC2 --> HCP_Eng
        Mgmt_DC2 --> HCP_Dev
        Mgmt_DC2 --> HCP_Member_DC2

        %% Hosted control planes to clusters
        HCP_Prod_DC1 --> Prod_DC1_Cluster
        HCP_NP1 --> NP1_Cluster
        HCP_NP2 --> NP2_Cluster
        HCP_Member_DC1 --> Member_DC1_Cluster

        HCP_Prod_DC2 --> Prod_DC2_Cluster
        HCP_Eng --> Eng_Cluster
        HCP_Dev --> Dev_Cluster
        HCP_Member_DC2 --> Member_DC2_Cluster

        %% Shared services connectivity
        Mgmt_DC1 --> Vault_Cluster
        Mgmt_DC2 --> Vault_Cluster
        Mgmt_DC1 --> Observability_Cluster
        Mgmt_DC2 --> Observability_Cluster
        Mgmt_DC1 --> ArgoCD_Cluster
        Mgmt_DC2 --> ArgoCD_Cluster
        Mgmt_DC1 --> Cert_Manager_Cluster
        Mgmt_DC2 --> Cert_Manager_Cluster

        %% Cross-DC and integration connectivity
        Mgmt_DC1 -.-> Mgmt_DC2
        Prod_DC1_Cluster -.-> Prod_DC2_Cluster
        Vault_Cluster -.-> Interconnect
        Observability_Cluster -.-> SIEM_Platform
        ArgoCD_Cluster -.-> ITSM_Platform


        %% External system integrations
        ESB --> ERP_Systems
        ESB --> CRM_Systems
        ESB --> Legacy_Systems
        iPaaS --> AWS_Services
        iPaaS --> Azure_Services
        iPaaS --> GCP_Services

        Analytics_Platform --> Observability_Cluster
        SIEM_Platform --> Vault_Cluster
    ```

    ##### Comprehensive System Context Analysis

    ###### Business Stakeholder Ecosystem
    - **Application Teams**: Multidisciplinary teams including developers, product owners, and analysts working collaboratively on application development and delivery
    - **End User Communities**: Diverse user base with different access patterns, security requirements, and service level expectations
    - **Partner Ecosystem**: B2B integrations requiring secure, reliable, and monitored access to platform services

    ###### Operations Team Structure
    - **Platform Operations**: Core platform management including infrastructure design, deployment automation, and operational excellence
    - **Infrastructure Operations**: Specialized teams for network, storage, systems, and database administration
    - **Support Teams**: 24/7 support for both application and infrastructure incidents, problems, and service requests

    ###### Enterprise Infrastructure Architecture
    - **Data Center Design**: Redundant, geographically distributed infrastructure with independent power, cooling, and network systems
    - **Network Architecture**: Multi-layer design with core, distribution, and access layers supporting advanced traffic engineering
    - **Security Architecture**: Defense-in-depth approach with multiple security controls and monitoring points
    - **Load Balancing Strategy**: Dual load balancing approach with A10 for primary traffic and F5 for advanced application delivery

    ###### OpenShift Platform Architecture
    - **Management Clusters**: Centralized control plane management with HyperShift for efficient resource utilization
    - **Hosted Control Planes**: Environment-specific control planes running as pods with complete isolation
    - **Hosted Clusters**: Production-ready environments with specific purposes, SLAs, and operational requirements
    - **Shared Services**: Enterprise-grade services providing centralized capabilities across all environments

    ###### Integration and Middleware Layer
    - **API Management**: Comprehensive API gateway and service mesh capabilities for microservices architecture
    - **Integration Platforms**: Enterprise service bus and iPaaS for complex system integrations
    - **Data Services**: Managed database, caching, and messaging services supporting application requirements

    ###### External Systems Integration
    - **Cloud Services**: Multi-cloud integration with AWS, Azure, and GCP for hybrid cloud capabilities
    - **Enterprise Applications**: Integration with ERP, CRM, and legacy systems through standardized interfaces
    - **Monitoring and Analytics**: Comprehensive observability and analytics platform integration

    ##### Key System Relationships and Data Flows

    1. **User Access Patterns**:
       - Internal users access via direct data center load balancers
       - External users leverage global load balancing for optimal performance
       - Partners use dedicated API gateways with enhanced security controls

    2. **Operational Workflows**:
       - Platform engineers manage infrastructure through management clusters
       - DevOps teams deploy applications via ArgoCD GitOps workflows
       - Security teams monitor and respond through integrated SIEM platforms

    3. **Infrastructure Management**:
       - Network operations manage load balancers and DNS infrastructure
       - Storage operations handle NetApp Trident and data services
       - Systems operations maintain physical and virtual infrastructure

    4. **Application Lifecycle**:
       - Development teams create applications in engineering/development environments
       - DevOps teams promote through testing environments (NP1/NP2, Member Test)
       - Production deployments use ArgoCD for automated, auditable releases

    5. **Monitoring and Observability**:
       - Centralized observability stack collects metrics from all environments
       - SIEM integration provides security monitoring and alerting
       - ITSM integration enables automated incident management

    6. **Disaster Recovery**:
       - Cross-DC replication ensures data availability
       - Global load balancing enables automatic failover
       - Management cluster redundancy provides control plane resilience

    #### C2 - Container Diagram (Environment Architecture)

    The C2 diagram provides a detailed view of the container architecture, illustrating how environments are structured, distributed across data centers, and interconnected. This view emphasizes the HyperShift architecture with hosted control planes and the multi-environment deployment strategy.

    ##### Comprehensive Container Architecture Diagram

    ```mermaid
    graph TB
        subgraph "DC1 - Primary Data Center Infrastructure"
            subgraph "Management Cluster DC1"
                subgraph "Control Plane Nodes"
                    CP1_DC1[Control Plane Node 1<br/>• 16 CPU, 64GB RAM<br/>• RHCOS 4.12+<br/>• etcd Member<br/>• API Server]
                    CP2_DC1[Control Plane Node 2<br/>• 16 CPU, 64GB RAM<br/>• RHCOS 4.12+<br/>• etcd Member<br/>• Controllers]
                    CP3_DC1[Control Plane Node 3<br/>• 16 CPU, 64GB RAM<br/>• RHCOS 4.12+<br/>• etcd Member<br/>• Scheduler]
                end

                subgraph "Infrastructure Worker Nodes"
                    Infra1_DC1[Infra Worker Node 1<br/>• 8 CPU, 32GB RAM<br/>• Monitoring Stack<br/>• Ingress Controllers<br/>• Logging Stack]
                    Infra2_DC1[Infra Worker Node 2<br/>• 8 CPU, 32GB RAM<br/>• Monitoring Stack<br/>• Ingress Controllers<br/>• Logging Stack]
                    Infra3_DC1[Infra Worker Node 3<br/>• 8 CPU, 32GB RAM<br/>• Monitoring Stack<br/>• Ingress Controllers<br/>• Logging Stack]
                    Infra4_DC1[Infra Worker Node 4<br/>• 8 CPU, 32GB RAM<br/>• ArgoCD Server<br/>• GitOps Operators<br/>• CI/CD Tools]
                    Infra5_DC1[Infra Worker Node 5<br/>• 8 CPU, 32GB RAM<br/>• ArgoCD Server<br/>• GitOps Operators<br/>• CI/CD Tools]
                    Infra6_DC1[Infra Worker Node 6<br/>• 8 CPU, 32GB RAM<br/>• ArgoCD Server<br/>• GitOps Operators<br/>• CI/CD Tools]
                end

                subgraph "HyperShift Operator"
                    HS_Operator_DC1[HyperShift Operator<br/>• Control Plane Provisioning<br/>• Cluster Lifecycle Mgmt<br/>• Hosted Cluster Creation<br/>• Resource Optimization]
                end
            end

            subgraph "Hosted Control Planes - Production DC1"
                subgraph "Production DC1 Control Plane"
                    HCP_Prod_DC1_API[API Server Pod<br/>• REST API Endpoint<br/>• Authentication<br/>• Authorization<br/>• Admission Control]
                    HCP_Prod_DC1_ETCD[etcd Pod<br/>• Key-Value Store<br/>• Cluster State<br/>• Consensus Protocol<br/>• Data Persistence]
                    HCP_Prod_DC1_Ctrl[Controller Pods<br/>• Deployment Controller<br/>• ReplicaSet Controller<br/>• HPA Controller<br/>• PVC Controller]
                    HCP_Prod_DC1_Sched[Scheduler Pod<br/>• Pod Scheduling<br/>• Resource Optimization<br/>• Affinity Rules<br/>• Node Selection]
                end
            end

            subgraph "Hosted Clusters - Production DC1"
                subgraph "Production DC1 Environment"
                    Prod_DC1_Workers[Worker Nodes<br/>• 6-12+ Nodes<br/>• 16 CPU, 64GB RAM each<br/>• Application Workloads<br/>• Multi-tenancy]
                    Prod_DC1_Storage[NetApp Trident<br/>• SSD Storage Classes<br/>• PVC Provisioning<br/>• Backup Integration<br/>• S3 Compatible]
                    Prod_DC1_Network[Network Layer<br/>• OpenShift SDN<br/>• Network Policies<br/>• Load Balancing]
                    Prod_DC1_Security[Security Layer<br/>• SCCs<br/>• Compliance Operator<br/>• File Integrity<br/>• Audit Logging]
                end
            end
        end

        subgraph "DC2 - Secondary Data Center Infrastructure"
            subgraph "Management Cluster DC2"
                subgraph "Control Plane Nodes"
                    CP1_DC2[Control Plane Node 1<br/>• 16 CPU, 64GB RAM<br/>• RHCOS 4.12+<br/>• etcd Member<br/>• API Server]
                    CP2_DC2[Control Plane Node 2<br/>• 16 CPU, 64GB RAM<br/>• RHCOS 4.12+<br/>• etcd Member<br/>• Controllers]
                    CP3_DC2[Control Plane Node 3<br/>• 16 CPU, 64GB RAM<br/>• RHCOS 4.12+<br/>• etcd Member<br/>• Scheduler]
                end

                subgraph "Infrastructure Worker Nodes"
                    Infra1_DC2[Infra Worker Node 1<br/>• 8 CPU, 32GB RAM<br/>• Monitoring Stack<br/>• Ingress Controllers<br/>• Logging Stack]
                    Infra2_DC2[Infra Worker Node 2<br/>• 8 CPU, 32GB RAM<br/>• Monitoring Stack<br/>• Ingress Controllers<br/>• Logging Stack]
                    Infra3_DC2[Infra Worker Node 3<br/>• 8 CPU, 32GB RAM<br/>• Monitoring Stack<br/>• Ingress Controllers<br/>• Logging Stack]
                    Infra4_DC2[Infra Worker Node 4<br/>• 8 CPU, 32GB RAM<br/>• ArgoCD Server<br/>• GitOps Operators<br/>• CI/CD Tools]
                    Infra5_DC2[Infra Worker Node 5<br/>• 8 CPU, 32GB RAM<br/>• ArgoCD Server<br/>• GitOps Operators<br/>• CI/CD Tools]
                    Infra6_DC2[Infra Worker Node 6<br/>• 8 CPU, 32GB RAM<br/>• ArgoCD Server<br/>• GitOps Operators<br/>• CI/CD Tools]
                end

                subgraph "HyperShift Operator"
                    HS_Operator_DC2[HyperShift Operator<br/>• Control Plane Provisioning<br/>• Cluster Lifecycle Mgmt<br/>• Hosted Cluster Creation<br/>• Resource Optimization]
                end
            end

            subgraph "Hosted Control Planes - Production DC2"
                subgraph "Production DC2 Control Plane"
                    HCP_Prod_DC2_API[API Server Pod<br/>• REST API Endpoint<br/>• Authentication<br/>• Authorization<br/>• Admission Control]
                    HCP_Prod_DC2_ETCD[etcd Pod<br/>• Key-Value Store<br/>• Cluster State<br/>• Consensus Protocol<br/>• Data Persistence]
                    HCP_Prod_DC2_Ctrl[Controller Pods<br/>• Deployment Controller<br/>• ReplicaSet Controller<br/>• HPA Controller<br/>• PVC Controller]
                    HCP_Prod_DC2_Sched[Scheduler Pod<br/>• Pod Scheduling<br/>• Resource Optimization<br/>• Affinity Rules<br/>• Node Selection]
                end
            end

            subgraph "Hosted Clusters - Production DC2"
                subgraph "Production DC2 Environment"
                    Prod_DC2_Workers[Worker Nodes<br/>• 6-12+ Nodes<br/>• 16 CPU, 64GB RAM each<br/>• Disaster Recovery<br/>• Failover Target<br/>• Read Replicas]
                    Prod_DC2_Storage[NetApp Trident<br/>• SSD Storage Classes<br/>• PVC Provisioning<br/>• Backup Integration<br/>• S3 Compatible]
                    Prod_DC2_Network[Network Layer<br/>• OpenShift SDN<br/>• Network Policies<br/>• Load Balancing]
                    Prod_DC2_Security[Security Layer<br/>• SCCs<br/>• Compliance Operator<br/>• File Integrity<br/>• Audit Logging]
                end
            end
        end

        subgraph "Shared Services & Cross-DC Infrastructure"
            subgraph "Centralized Services"
                Vault_Cluster[HashiCorp Vault Enterprise<br/>• Multi-DC Replication<br/>• HSM Integration<br/>• Enterprise Features<br/>• Audit Logging<br/>• Dynamic Secrets<br/>• Certificate Authority]
                Observability_Cluster[Grafana + Loki + Mimir + Alloy<br/>• Federated Metrics<br/>• Cross-Cluster Logs<br/>• Long-term Storage<br/>• Global Dashboards<br/>• Alerting Engine<br/>• Distributed Tracing]
                ArgoCD_Cluster[ArgoCD Enterprise<br/>• Multi-environment Sync<br/>• Cross-DC Deployments<br/>• Rollback Automation<br/>• Access Control<br/>• SSO Integration<br/>• Audit Trails]
                Cert_Manager_Cluster[cert-manager + Vault<br/>• Enterprise CA Integration<br/>• Automated Renewal<br/>• Certificate Monitoring<br/>• Compliance Reporting<br/>• Multi-domain Support]
            end

            subgraph "Cross-DC Connectivity"
                Interconnect[Layer 3 Interconnect<br/>• BGP Routing<br/>• VPN Tunnels<br/>• Direct Connect<br/>• Sub-5ms Latency<br/>• 10Gbps Bandwidth<br/>• Redundant Paths]
                GlobalDNS[Global DNS Infrastructure<br/>• Geo-DNS<br/>• Health Checks<br/>• Load Balancing<br/>• Failover Automation<br/>• Anycast Routing]
                GlobalLB[Global Load Balancing<br/>• Traffic Steering<br/>• Performance Routing<br/>• Disaster Recovery<br/>• Auto-scaling<br/>• SSL Orchestration]
            end

            subgraph "Data Replication & Backup"
                DataReplication[Cross-DC Data Replication<br/>• NetApp SnapMirror<br/>• Real-time Sync<br/>• RPO < 5 minutes<br/>• Multi-site Consistency<br/>• Automated Failover]
                BackupSystems[Enterprise Backup<br/>• NetApp SnapCenter<br/>• Cross-DC Backups<br/>• Long-term Retention<br/>• Compliance Archiving<br/>• Disaster Recovery]
                DR_Coordination[DR Coordination<br/>• Automated Failover<br/>• Runbook Automation<br/>• Communication Plans<br/>• Recovery Testing<br/>• Business Continuity]
            end
        end

        %% Management cluster internal connections
        CP1_DC1 --> HS_Operator_DC1
        CP2_DC1 --> HS_Operator_DC1
        CP3_DC1 --> HS_Operator_DC1
        Infra1_DC1 --> HS_Operator_DC1
        Infra2_DC1 --> HS_Operator_DC1
        Infra3_DC1 --> HS_Operator_DC1

        CP1_DC2 --> HS_Operator_DC2
        CP2_DC2 --> HS_Operator_DC2
        CP3_DC2 --> HS_Operator_DC2
        Infra1_DC2 --> HS_Operator_DC2
        Infra2_DC2 --> HS_Operator_DC2
        Infra3_DC2 --> HS_Operator_DC2

        %% HyperShift operator to hosted control planes
        HS_Operator_DC1 --> HCP_Prod_DC1_API
        HS_Operator_DC2 --> HCP_Prod_DC2_API

        %% Hosted control plane internal connections
        HCP_Prod_DC1_API --> HCP_Prod_DC1_ETCD
        HCP_Prod_DC1_API --> HCP_Prod_DC1_Ctrl
        HCP_Prod_DC1_API --> HCP_Prod_DC1_Sched
        HCP_Prod_DC1_ETCD --> HCP_Prod_DC1_Ctrl
        HCP_Prod_DC1_Ctrl --> HCP_Prod_DC1_Sched

        HCP_Prod_DC2_API --> HCP_Prod_DC2_ETCD
        HCP_Prod_DC2_API --> HCP_Prod_DC2_Ctrl
        HCP_Prod_DC2_API --> HCP_Prod_DC2_Sched
        HCP_Prod_DC2_ETCD --> HCP_Prod_DC2_Ctrl
        HCP_Prod_DC2_Ctrl --> HCP_Prod_DC2_Sched

        %% Hosted control planes to hosted clusters
        HCP_Prod_DC1_API --> Prod_DC1_Workers
        HCP_Prod_DC2_API --> Prod_DC2_Workers

        %% Environment component connections
        Prod_DC1_Workers --> Prod_DC1_Storage
        Prod_DC1_Workers --> Prod_DC1_Network
        Prod_DC1_Workers --> Prod_DC1_Security
        Prod_DC1_Storage --> Prod_DC1_Network
        Prod_DC1_Network --> Prod_DC1_Security

        Prod_DC2_Workers --> Prod_DC2_Storage
        Prod_DC2_Workers --> Prod_DC2_Network
        Prod_DC2_Workers --> Prod_DC2_Security
        Prod_DC2_Storage --> Prod_DC2_Network
        Prod_DC2_Network --> Prod_DC2_Security

        %% Shared services connectivity
        Infra1_DC1 --> Vault_Cluster
        Infra2_DC1 --> Vault_Cluster
        Infra4_DC1 --> ArgoCD_Cluster
        Infra5_DC1 --> ArgoCD_Cluster
        Infra1_DC1 --> Observability_Cluster
        Infra2_DC1 --> Observability_Cluster
        Infra3_DC1 --> Observability_Cluster

        Infra1_DC2 --> Vault_Cluster
        Infra2_DC2 --> Vault_Cluster
        Infra4_DC2 --> ArgoCD_Cluster
        Infra5_DC2 --> ArgoCD_Cluster
        Infra1_DC2 --> Observability_Cluster
        Infra2_DC2 --> Observability_Cluster
        Infra3_DC2 --> Observability_Cluster

        %% Cross-DC connections
        CP1_DC1 -.-> CP1_DC2
        HS_Operator_DC1 -.-> HS_Operator_DC2
        Vault_Cluster -.-> Interconnect
        Observability_Cluster -.-> Interconnect
        ArgoCD_Cluster -.-> Interconnect
        Cert_Manager_Cluster -.-> Interconnect

        %% Global infrastructure connections
        Interconnect -.-> GlobalDNS
        GlobalDNS -.-> GlobalLB
        GlobalLB -.-> DataReplication
        DataReplication -.-> BackupSystems
        BackupSystems -.-> DR_Coordination

        %% Environment-specific monitoring connections
        Prod_DC1_Security -.-> Vault_Cluster
        Prod_DC2_Security -.-> Vault_Cluster
    ```

    ##### Detailed Environment Architecture Analysis

    ###### Management Cluster Architecture

    **Control Plane Nodes:**
    - **Hardware Specification**: 16 CPU cores, 64GB RAM, high-performance SSD storage
    - **Operating System**: Red Hat Enterprise Linux CoreOS 4.12+
    - **etcd Configuration**: Distributed consensus with 3-node cluster for high availability
    - **API Server**: RESTful API endpoint with comprehensive security controls
    - **Controller Manager**: Implements control loops for desired state management
    - **Scheduler**: Intelligent pod placement with resource optimization

    **Infrastructure Worker Nodes:**
    - **Resource Allocation**: 8 CPU cores, 32GB RAM for infrastructure workloads
    - **Workload Distribution**: Dedicated nodes for monitoring, logging, and ingress
    - **ArgoCD Deployment**: GitOps server and operators on dedicated nodes
    - **Resource Isolation**: Node selectors and taints for workload separation

    **HyperShift Operator:**
    - **Functionality**: Automated provisioning of hosted control planes
    - **Resource Management**: Efficient allocation of compute resources
    - **Lifecycle Management**: Creation, scaling, and deletion of hosted clusters
    - **Integration**: Seamless integration with management cluster services

    ###### Hosted Control Planes

    **Architecture Pattern:**
    - **Pod-based Control Planes**: Each environment has dedicated control plane pods
    - **Namespace Isolation**: Complete separation between environment control planes
    - **Resource Optimization**: Shared underlying infrastructure with dedicated resources
    - **High Availability**: Distributed components across multiple nodes

    **Component Breakdown:**
    - **API Server Pod**: REST API endpoint with authentication and authorization
    - **etcd Pod**: Distributed key-value store for cluster state persistence
    - **Controller Pods**: Kubernetes controllers for workload management
    - **Scheduler Pod**: Pod scheduling with affinity and anti-affinity rules

    ###### Environment-Specific Configurations

    **Production Environments:**
    - **Scale**: 6-12+ worker nodes with horizontal scaling capabilities
    - **Storage**: NetApp Trident with SSD storage classes and backup integration
    - **Networking**: Advanced service mesh with network policies and load balancing
    - **Security**: Enhanced security context constraints and compliance monitoring
    - **SLA**: 99.9% uptime with automated failover capabilities

    **Non-Production Testing Environments (NP1/NP2):**
    - **NP1 - Performance Testing**:
      - Specialized tools: JMeter, K6, Locust for load generation
      - Detailed metrics collection for performance analysis
      - Network isolation for accurate testing conditions
      - Integration with enterprise monitoring systems

    - **NP2 - Security Testing**:
      - Security scanning tools: OWASP ZAP, Nessus, SonarQube
      - Compliance checking with CIS benchmarks
      - Vulnerability assessment and penetration testing
      - Isolated network segments for security testing

    **Development Environments:**
    - **Engineering Environment**:
      - Flexible configuration for prototyping and experimentation
      - Development tools and IDE integration
      - Elevated permissions for development workflows
      - Innovation sandbox with experimental features

    - **Development Environment**:
      - CI/CD pipeline integration and validation
      - Automated testing and code quality gates
      - Integration testing across multiple components
      - Quality monitoring and reporting

    **Member Test Environments:**
    - **Configuration**: Production-like data sets and configurations
    - **Validation**: Member-specific testing scenarios and requirements
    - **Integration**: Cross-DC testing capabilities and performance validation
    - **Isolation**: Dedicated network segments and security groups

    ###### Cross-DC Infrastructure

    **Layer 3 Interconnect:**
    - **Routing**: BGP routing with redundant paths and automatic failover
    - **Connectivity**: VPN tunnels and direct connect for secure communication
    - **Performance**: Sub-5ms latency with 10Gbps bandwidth guarantees
    - **Reliability**: Multiple redundant paths with automatic traffic engineering

    **Global Load Balancing:**
    - **Traffic Steering**: Intelligent routing based on performance and availability
    - **Health Monitoring**: Continuous health checks and automatic failover
    - **SSL Orchestration**: Centralized SSL termination and certificate management
    - **Geo-Distribution**: Anycast routing for optimal user experience

    **Data Replication and Backup:**
    - **NetApp SnapMirror**: Real-time data replication with RPO < 5 minutes
    - **Cross-DC Backups**: Automated backup systems with long-term retention
    - **Disaster Recovery**: Coordinated failover with automated runbooks
    - **Business Continuity**: Comprehensive business continuity planning

    #### C3 - Component Diagram (Technical Architecture)

    The C3 diagram provides detailed insight into the internal components, showing how individual technologies interact within the HyperShift architecture.

    ##### Diagram

    ```mermaid
    graph TB
        subgraph "Management Cluster Components"
            subgraph "Control Plane Stack"
                API_Server[API Server<br/>• REST API<br/>• Authentication<br/>• Authorization<br/>• Admission Control]
                ETCD[etcd Cluster<br/>• Key-Value Store<br/>• Distributed Consensus<br/>• Data Persistence<br/>• Backup/Restore]
                Controllers[Kubernetes Controllers<br/>• Deployment Controller<br/>• ReplicaSet Controller<br/>• HPA Controller<br/>• Job Controller]
                Scheduler[Kubernetes Scheduler<br/>• Pod Scheduling<br/>• Resource Optimization<br/>• Affinity/Anti-affinity<br/>• Node Selection]
            end

            subgraph "Platform Operators"
                Ingress_Operator[Ingress Operator<br/>• Route Management<br/>• HAProxy Config<br/>• SSL Termination<br/>• Load Balancing]
                Monitoring_Operator[Monitoring Operator<br/>• Prometheus Deployment<br/>• Alertmanager Config<br/>• Grafana Setup<br/>• Metrics Collection]
                Logging_Operator[Logging Operator<br/>• Loki Stack<br/>• Fluentd Config<br/>• Log Aggregation<br/>• Retention Policies]
                Network_Operator[Network Operator<br/>• SDN Configuration<br/>• Network Policies<br/>• Multus CNI]
                Storage_Operator[Storage Operator<br/>• NetApp Trident<br/>• CSI Drivers<br/>• Storage Classes<br/>• Volume Management]
                Auth_Operator[Authentication Operator<br/>• OAuth Integration<br/>• Identity Providers<br/>• RBAC Policies<br/>• Token Management]
            end

            subgraph "Shared Services"
                Vault_Instance[HashiCorp Vault<br/>• Secrets Engine<br/>• PKI Backend<br/>• Authentication<br/>• Audit Logging]
                Cert_Manager[cert-manager<br/>• Certificate Issuance<br/>• Renewal Automation<br/>• ACME Integration<br/>• Secret Management]
                ArgoCD_Instance[ArgoCD<br/>• Application Controller<br/>• Repository Server<br/>• Dex Integration<br/>• RBAC Policies]
            end

            subgraph "Observability Stack"
                Prometheus_Instance[Prometheus<br/>• Metrics Collection<br/>• Service Discovery<br/>• Alerting Rules<br/>• Federation]
                Grafana_Instance[Grafana<br/>• Dashboard Creation<br/>• Data Sources<br/>• Alerting<br/>• User Management]
                Loki_Instance[Loki<br/>• Log Ingestion<br/>• Query Language<br/>• Retention<br/>• Multi-tenancy]
                Mimir_Instance[Mimir<br/>• Long-term Storage<br/>• Query Optimization<br/>• Downsampling<br/>• Replication]
                Alloy_Instance[Alloy<br/>• Metrics Forwarding<br/>• Log Shipping<br/>• Tracing<br/>• Configuration]
            end
        end

        subgraph "Hosted Cluster Components"
            subgraph "Hosted Control Plane"
                Hosted_API[Hosted API Server<br/>• Environment-specific<br/>• Isolated Control Plane<br/>• Custom Admission<br/>• RBAC Policies]
                Hosted_ETCD[Hosted etcd<br/>• Environment Data<br/>• Independent Persistence<br/>• Backup Integration<br/>• Performance Optimized]
                Hosted_Controllers[Hosted Controllers<br/>• Environment Controllers<br/>• Custom Resources<br/>• Operator Integration<br/>• Lifecycle Management]
            end

            subgraph "Worker Node Layer"
                Kubelet[Kubelet<br/>• Pod Lifecycle<br/>• Resource Management<br/>• CRI Interface<br/>• Node Status]
                CRI_O[CRI-O<br/>• Container Runtime<br/>• Image Management<br/>• Security<br/>• Performance]
                CNI_Plugins[CNI Plugins<br/>• Network Setup<br/>• IPAM<br/>• Policy Enforcement]
                Device_Plugins[Device Plugins<br/>• GPU Support<br/>• Network Interfaces<br/>• Storage Devices<br/>• Hardware Acceleration]
            end

            subgraph "Application Runtime"
                Ingress_Controllers[Ingress Controllers<br/>• HAProxy Ingress<br/>• Nginx Ingress<br/>• SSL Termination<br/>• Path-based Routing]
                Application_Operators[Application Operators<br/>• GitOps Operator]
            end
        end

        subgraph "Storage & Networking Infrastructure"
            subgraph "Storage Layer"
                NetApp_Trident[NetApp Trident<br/>• CSI Driver<br/>• ONTAP Integration<br/>• S3 Compatibility<br/>• Multi-protocol]
                Storage_Classes[Storage Classes<br/>• SSD Performance<br/>• HDD Capacity<br/>• Backup Classes<br/>• Encryption]
                PVCs[Persistent Volumes<br/>• Dynamic Provisioning<br/>• Storage Optimization<br/>• Backup Integration<br/>• Multi-attach]
            end

            subgraph "Network Layer"
                MetalLB[MetalLB<br/>• BGP Announcements<br/>• L2/L3 Modes<br/>• Load Balancing<br/>• Service Exposure]
                External_LB[External Load Balancers<br/>• A10 Thunder<br/>• F5 BIG-IP<br/>• SSL Offloading<br/>• Global Balancing]
                SDN[Software Defined Network<br/>• OpenShift SDN<br/>• Network Policies<br/>• Egress Control<br/>• Multitenancy]
            end
        end

        subgraph "Security & Compliance"
            subgraph "Security Components"
                SCCs[Security Context Constraints<br/>• Pod Security<br/>• Privilege Control<br/>• User Permissions<br/>• Runtime Security]
                Compliance_Operator[Compliance Operator<br/>• CIS Benchmarks<br/>• Security Scanning<br/>• Remediation<br/>• Audit Reports]
                File_Integrity[File Integrity Operator<br/>• AIDE Integration<br/>• Change Detection<br/>• Alerting<br/>• Compliance]
            end

            subgraph "Secret Management"
                Vault_Secrets_Operator[Vault Secrets Operator<br/>• Secret Injection<br/>• Dynamic Secrets<br/>• Lease Management<br/>• Rotation]
                External_Secrets[External Secrets Operator<br/>• Cross-platform<br/>• Secret Sync<br/>• Transformation<br/>• Backup]
            end
        end

        %% Control plane interactions
        API_Server --> ETCD
        API_Server --> Controllers
        API_Server --> Scheduler
        Controllers --> ETCD
        Scheduler --> ETCD

        %% Operator to component relationships
        Ingress_Operator --> Ingress_Controllers
        Monitoring_Operator --> Prometheus_Instance
        Monitoring_Operator --> Grafana_Instance
        Logging_Operator --> Loki_Instance
        Network_Operator --> SDN
        Storage_Operator --> NetApp_Trident
        Auth_Operator --> API_Server

        %% Shared services integration
        Vault_Instance --> Cert_Manager
        Cert_Manager --> Ingress_Controllers
        ArgoCD_Instance --> Application_Operators

        %% Observability data flow
        Prometheus_Instance --> Mimir_Instance
        Alloy_Instance --> Prometheus_Instance
        Alloy_Instance --> Loki_Instance
        Loki_Instance --> Grafana_Instance
        Mimir_Instance --> Grafana_Instance

        %% Hosted cluster architecture
        Hosted_API --> Hosted_ETCD
        Hosted_API --> Hosted_Controllers
        Hosted_Controllers --> Kubelet
        Kubelet --> CRI_O
        CRI_O --> CNI_Plugins

        %% Storage integration
        NetApp_Trident --> Storage_Classes
        Storage_Classes --> PVCs
        PVCs --> Application_Operators

        %% Network integration
        MetalLB --> External_LB
        SDN --> MetalLB
        Ingress_Controllers --> External_LB

        %% Security integration
        SCCs --> Kubelet
        Compliance_Operator --> SCCs
        Vault_Secrets_Operator --> Vault_Instance
        Vault_Secrets_Operator --> Application_Operators
    ```

    ##### Component Interaction Details

    ###### Control Plane Stack
    - **API Server**: Central component handling all API requests with comprehensive security controls
    - **etcd**: Distributed key-value store with consensus algorithm for data consistency
    - **Controllers**: Implement control loops for maintaining desired state
    - **Scheduler**: Intelligent pod placement based on resource requirements and constraints

    ###### Platform Operators
    - **Ingress Operator**: Manages external access with advanced routing and SSL capabilities
    - **Monitoring Operator**: Deploys and configures the complete observability stack
    - **Logging Operator**: Manages log collection, aggregation, and retention
    - **Network Operator**: Configures networking, policies, and service mesh integration
    - **Storage Operator**: Integrates with NetApp Trident for enterprise storage
    - **Authentication Operator**: Manages identity integration and access control

    ###### Shared Services Integration
    - **HashiCorp Vault**: Centralized secrets management with enterprise features
    - **cert-manager**: Automated certificate lifecycle management
    - **ArgoCD**: GitOps deployment engine with multi-environment support

    ###### Observability Stack
    - **Prometheus**: Metrics collection with service discovery and alerting
    - **Grafana**: Visualization platform with advanced dashboarding
    - **Loki**: Log aggregation system optimized for Kubernetes
    - **Mimir**: Long-term metrics storage with high availability
    - **Alloy**: Unified telemetry collector for metrics, logs, and traces

    ###### Storage & Networking
    - **NetApp Trident**: Enterprise storage orchestration with multi-protocol support
    - **MetalLB**: In-cluster load balancing for Kubernetes services
    - **External Load Balancers**: A10 and F5 for enterprise traffic management

    ## Deployment Procedures

### Infrastructure Setup

#### 1. Network Configuration

Configure the following network segments:

```yaml
# Example network configuration
networks:
  - name: management
    vlan: 100
    subnet: 10.0.100.0/24
    gateway: 10.0.100.1

  - name: hosted-clusters
    vlan: 200
    subnet: 10.0.200.0/22
    gateway: 10.0.200.1

  - name: storage
    vlan: 300
    subnet: 10.0.300.0/24
    gateway: 10.0.300.1
````

#### 2. DNS Configuration

Set up DNS records for all clusters:

```bash
# Management cluster DNS
api.mgmt-dc1.example.com    A    10.0.100.10
apps.mgmt-dc1.example.com   A    10.0.100.11

# Hosted cluster DNS (example for production)
api.prod-dc1.example.com    A    10.0.200.10
apps.prod-dc1.example.com   A    10.0.200.11
```

#### 3. Load Balancer Configuration

##### In-Cluster Load Balancing (MetalLB)

Configure MetalLB for services requiring load balancing within the cluster:

```bash
# Install MetalLB operator
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: metallb-operator
  namespace: metallb-system
spec:
  channel: stable
  name: metallb-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF

# Configure MetalLB address pools
oc apply -f - <<EOF
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: service-pool
  namespace: metallb-system
spec:
  addresses:
  - 10.0.100.100-10.0.100.200
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: l2-advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
  - service-pool
EOF
```

##### External Load Balancing (A10/F5)

External traffic to OpenShift clusters is managed by enterprise-grade A10 Thunder and F5 BIG-IP load balancers:

- **A10 Thunder**: Primary load balancer for most traffic patterns
- **F5 BIG-IP**: Used for advanced traffic management, SSL offloading, and application delivery

**Configuration Overview:**

- API server traffic (port 6443) load balanced across control plane nodes
- Ingress traffic (ports 80/443) routed through ingress controllers
- Health checks configured for automatic failover
- SSL/TLS termination handled at the load balancer layer

**Load Balancer VIPs:**

- Management Cluster DC1: `api.mgmt-dc1.example.com` → A10 VIP
- Hosted Clusters: `*.apps.example.com` → F5 VIP pool

### Management Cluster Deployment

#### Prerequisites

- 3 control plane nodes (16 CPU, 64GB RAM each)
- 3 worker nodes (8 CPU, 32GB RAM each)
- Shared storage configured
- Load balancer configured

#### Deployment Steps

1. **Prepare Installation Configuration**

```bash
# Create installation directory
mkdir ~/ocp-install
cd ~/ocp-install

# Download OpenShift installer
wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/openshift-install-linux.tar.gz
tar -xzf openshift-install-linux.tar.gz

# Create install-config.yaml
cat > install-config.yaml << EOF
apiVersion: v1
baseDomain: example.com
compute:
- hyperthreading: Enabled
  name: worker
  replicas: 3
controlPlane:
  hyperthreading: Enabled
  name: master
  replicas: 3
metadata:
  name: mgmt-dc1
networking:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  networkType: OpenShiftSDN
  serviceNetwork:
  - 172.30.0.0/16
platform:
  baremetal:
    apiVIP: 10.0.100.10
    ingressVIP: 10.0.100.11
    provisioningNetwork: "Disabled"
pullSecret: '<pull_secret>'
sshKey: '<ssh_public_key>'
EOF
```

2. **Deploy Management Cluster**

```bash
# Create manifests
./openshift-install create manifests --dir=.

# Deploy cluster
./openshift-install create cluster --dir=.

# Monitor installation
./openshift-install wait-for install-complete --dir=.
```

3. **Post-Installation Configuration**

```bash
# Login to cluster
oc login -u kubeadmin -p $(cat auth/kubeadmin-password) https://api.mgmt-dc1.example.com:6443

# Install HyperShift operator
oc apply -f https://raw.githubusercontent.com/openshift/hypershift/main/cmd/install/assets/hypershift_operator.yaml

# Wait for operator to be ready
oc wait --for=condition=Available --timeout=10m deployment/hypershift-operator -n hypershift
```

### Hosted Cluster Provisioning

#### Create a Hosted Cluster

```bash
# Create hosted cluster namespace
oc create namespace clusters

# Create hosted cluster
oc process -f templates/hosted-cluster.yml \
  -p CLUSTER_NAME=production \
  -p BASE_DOMAIN=example.com \
  -p NODE_COUNT=6 \
  -p ENVIRONMENT=prod | oc apply -f -

# Wait for cluster to be ready
oc wait --for=condition=Available --timeout=30m hostedcluster/production -n clusters
```

#### Configure Worker Nodes

```bash
# Get hosted cluster kubeconfig
oc get secret/production-admin-kubeconfig -n clusters -o jsonpath='{.data.kubeconfig}' | base64 -d > production.kubeconfig

# Add worker nodes (using machine sets)
oc --kubeconfig=production.kubeconfig apply -f machine-sets/worker-machineset.yml

# Verify nodes
oc --kubeconfig=production.kubeconfig get nodes
```

## Configuration Management

### Operator Overview

#### OpenShift Operator Ecosystem Architecture

```mermaid
graph TB
    subgraph "Operator Lifecycle Management"
        OLM[Operator Lifecycle Manager<br/>• Manages operator installations<br/>• Handles updates and upgrades<br/>• Dependency resolution<br/>• Catalog management]
        OperatorHub[OperatorHub<br/>• Centralized operator catalog<br/>• Community and certified operators<br/>• Search and discovery<br/>• Version management]
    end

    subgraph "Platform Infrastructure Operators"
        MachineAPI[Machine API Operator<br/>• Node provisioning<br/>• Auto-scaling<br/>• Machine health checks<br/>• Cloud provider integration]
        ClusterVersion[Cluster Version Operator<br/>• OpenShift updates<br/>• Version management<br/>• Rollback capabilities<br/>• Compatibility validation]
        ClusterAutoscaler[Cluster Autoscaler<br/>• Dynamic node scaling<br/>• Resource optimization<br/>• Cost management<br/>• Performance scaling]
        IngressOperator[Ingress Operator<br/>• Route management<br/>• HAProxy configuration<br/>• SSL termination<br/>• Load balancing]
        MonitoringOperator[Monitoring Operator<br/>• Prometheus deployment<br/>• Alertmanager setup<br/>• Grafana integration<br/>• Metrics collection]
        LoggingOperator[Logging Operator<br/>• Loki stack management<br/>• Fluentd configuration<br/>• Log aggregation<br/>• Retention policies]
        NetworkOperator[Network Operator<br/>• OpenShift SDN<br/>• OVN-Kubernetes<br/>• Network policies<br/>• Service mesh integration]
        StorageOperator[Storage Operator<br/>• NetApp Trident<br/>• CSI drivers<br/>• Storage classes<br/>• Volume management]
        AuthenticationOperator[Authentication Operator<br/>• OAuth server<br/>• Identity providers<br/>• LDAP/AD integration<br/>• Token management]
        DNSOperator[DNS Operator<br/>• CoreDNS configuration<br/>• External DNS<br/>• DNSSEC support<br/>• Load balancing]
    end

    subgraph "Application Platform Operators"
        GitOpsOperator[GitOps Operator<br/>• ArgoCD integration<br/>• Application sync<br/>• Drift detection<br/>• Rollback automation]
        ComplianceOperator[Compliance Operator<br/>• CIS benchmarks<br/>• Security scanning<br/>• Remediation<br/>• Audit reporting]
        FileIntegrityOperator[File Integrity Operator<br/>• AIDE monitoring<br/>• Change detection<br/>• Security alerts<br/>• Compliance]
    end

    subgraph "Third-Party Operators"
        ArgoCDOperator[ArgoCD Operator<br/>• GitOps deployments<br/>• ApplicationSets<br/>• Image updating<br/>• Multi-environment sync]
        VaultOperator[Vault Secrets Operator<br/>• Secret injection<br/>• Dynamic secrets<br/>• Lease management<br/>• RBAC integration]
        CertManagerOperator[cert-manager Operator<br/>• Certificate issuance<br/>• ACME integration<br/>• Vault PKI backend<br/>• Renewal automation]
        MetalLBOperator[MetalLB Operator<br/>• Load balancing<br/>• BGP announcements<br/>• L2/L3 modes<br/>• Service exposure]
        TridentOperator[NetApp Trident Operator<br/>• Storage orchestration<br/>• ONTAP integration<br/>• S3 compatibility<br/>• Multi-protocol support]
        GrafanaOperator[Grafana Operator<br/>• Dashboard management<br/>• Data source config<br/>• User management<br/>• SSO integration]
        LokiOperator[Loki Operator<br/>• Log aggregation<br/>• Query engine<br/>• Retention policies<br/>• Multi-tenancy]
        MimirOperator[Mimir Operator<br/>• Long-term metrics<br/>• Query optimization<br/>• Downsampling<br/>• High availability]
    end

    subgraph "Operator Dependencies & Interactions"
        OLM --> OperatorHub
        OperatorHub --> MachineAPI
        OperatorHub --> ClusterVersion
        OperatorHub --> ClusterAutoscaler
        OperatorHub --> IngressOperator
        OperatorHub --> MonitoringOperator
        OperatorHub --> LoggingOperator
        OperatorHub --> NetworkOperator
        OperatorHub --> StorageOperator
        OperatorHub --> AuthenticationOperator
        OperatorHub --> DNSOperator
        OperatorHub --> GitOpsOperator
        OperatorHub --> ComplianceOperator
        OperatorHub --> FileIntegrityOperator

        OperatorHub --> ArgoCDOperator
        OperatorHub --> VaultOperator
        OperatorHub --> CertManagerOperator
        OperatorHub --> MetalLBOperator
        OperatorHub --> TridentOperator
        OperatorHub --> GrafanaOperator
        OperatorHub --> LokiOperator
        OperatorHub --> MimirOperator

        %% Cross-operator dependencies
        MonitoringOperator --> GrafanaOperator
        LoggingOperator --> LokiOperator
        MonitoringOperator --> MimirOperator
        StorageOperator --> TridentOperator
        IngressOperator --> MetalLBOperator
        GitOpsOperator --> ArgoCDOperator
        VaultOperator --> CertManagerOperator
        AuthenticationOperator --> VaultOperator
    end

    subgraph "Operator Management & Monitoring"
        OperatorMetrics[Operator Metrics<br/>• Health monitoring<br/>• Performance metrics<br/>• Resource usage<br/>• Error tracking]
        OperatorLifecycle[Operator Lifecycle<br/>• Installation<br/>• Updates<br/>• Upgrades<br/>• Uninstallation]
        OperatorRBAC[Operator RBAC<br/>• Permission management<br/>• Access control<br/>• Security policies<br/>• Audit logging]
    end

    %% Management connections
    OLM --> OperatorMetrics
    OLM --> OperatorLifecycle
    OLM --> OperatorRBAC

    MachineAPI --> OperatorMetrics
    ClusterVersion --> OperatorMetrics
    ArgoCDOperator --> OperatorMetrics
    VaultOperator --> OperatorMetrics
```

##### Operator Categories and Responsibilities

| Category                 | Purpose                              | Key Operators                                | Update Frequency                      |
| ------------------------ | ------------------------------------ | -------------------------------------------- | ------------------------------------- |
| **Platform Core**        | Essential cluster functionality      | Machine API, Cluster Version, Authentication | Critical - immediate updates          |
| **Infrastructure**       | Network, storage, ingress management | Network, Storage, Ingress, DNS               | High - weekly updates                 |
| **Observability**        | Monitoring, logging, tracing         | Monitoring, Logging                          | High - bi-weekly updates              |
| **Security**             | Compliance, secrets, access control  | Compliance, File Integrity, Vault            | Critical - immediate security updates |
| **Application Platform** | Development and deployment tools     | GitOps                                       | Medium - monthly updates              |
| **Third-Party**          | External tool integration            | ArgoCD, Grafana, Trident                     | Medium - monthly updates              |

##### Operator Update Strategy

```mermaid
flowchart TD
    A[New Operator Version Available] --> B{Impact Assessment}
    B --> C{Critical Security Update?}
    C --> D[Immediate Update Required]
    C --> E{Platform Compatibility?}
    E --> F[Schedule Update Window]
    E --> G[Defer Update - Test Compatibility]

    D --> H[Create Update Plan]
    F --> H
    G --> I[Monitor Compatibility Testing]

    H --> J[Backup Current State]
    J --> K[Update in Staging Environment]
    K --> L{Testing Successful?}
    L --> M[Update Production Environment]
    L --> N[Rollback and Investigate]

    M --> O[Monitor Post-Update]
    O --> P{Stable Operation?}
    P --> Q[Update Complete]
    P --> R[Rollback Required]

    I --> S[Compatibility Resolved]
    S --> H

    N --> T[Fix Issues]
    T --> K

    R --> U[Execute Rollback]
    U --> V[Post-Mortem Analysis]
    V --> W[Update Process Documentation]
```

##### Operator Health Monitoring Dashboard

```mermaid
graph TB
    subgraph "Operator Health Dashboard"
        HealthOverview[Health Overview<br/>• Overall operator status<br/>• Critical alerts<br/>• Update availability<br/>• Resource utilization]

        OperatorStatus[Operator Status Grid<br/>• Individual operator health<br/>• Version information<br/>• Last update time<br/>• Error conditions]

        DependencyMap[Dependency Visualization<br/>• Operator relationships<br/>• Impact analysis<br/>• Update sequencing<br/>• Failure propagation]
    end

    subgraph "Monitoring Integration"
        PrometheusMetrics[Prometheus Metrics<br/>• Operator custom metrics<br/>• Health check endpoints<br/>• Performance indicators<br/>• Error rates]

        AlertManager[AlertManager Rules<br/>• Operator failure alerts<br/>• Update failure notifications<br/>• Resource threshold alerts<br/>• Security vulnerability alerts]

        GrafanaDashboards[Grafana Dashboards<br/>• Operator health panels<br/>• Performance graphs<br/>• Update tracking<br/>• Resource usage trends]
    end

    subgraph "Automated Responses"
        AutoHealing[Auto-healing Actions<br/>• Pod restarts<br/>• Operator redeployment<br/>• Configuration fixes<br/>• Resource scaling]

        NotificationSystem[Notification System<br/>• Slack/Teams alerts<br/>• Email notifications<br/>• PagerDuty integration<br/>• Escalation procedures]
    end

    HealthOverview --> PrometheusMetrics
    OperatorStatus --> AlertManager
    DependencyMap --> GrafanaDashboards

    PrometheusMetrics --> AutoHealing
    AlertManager --> NotificationSystem
    GrafanaDashboards --> NotificationSystem
```

##### Operator Troubleshooting Decision Tree

```mermaid
flowchart TD
    A[Operator Issue Detected] --> B{What type of issue?}
    B --> C[Operator Not Starting]
    B --> D[Operator Degraded]
    B --> E[Operator Updates Failing]
    B --> F[Operator Resource Issues]

    C --> C1[Check operator subscription]
    C1 --> C2{Subscription valid?}
    C2 --> C3[Check cluster resources]
    C2 --> C4[Fix subscription]

    C3 --> C5{Resources available?}
    C5 --> C6[Check operator logs]
    C5 --> C7[Scale up cluster]

    C6 --> C8{Logs show errors?}
    C8 --> C9[Fix configuration]
    C8 --> C10[Contact support]

    D --> D1[Check operator status]
    D1 --> D2{Status shows errors?}
    D2 --> D3[Check dependencies]
    D2 --> D4[Restart operator]

    D3 --> D5{Dependencies healthy?}
    D5 --> D6[Check operator version]
    D5 --> D7[Fix dependencies]

    E --> E1[Check update channel]
    E1 --> E2{Channel correct?}
    E2 --> E3[Check approval strategy]
    E2 --> E4[Change channel]

    E3 --> E5{Approval automatic?}
    E5 --> E6[Check update path]
    E5 --> E7[Approve manually]

    F --> F1[Check resource limits]
    F1 --> F2{Limits appropriate?}
    F2 --> F3[Check cluster capacity]
    F2 --> F4[Adjust limits]

    F3 --> F5{Capacity sufficient?}
    F5 --> F6[Check operator metrics]
    F5 --> F7[Scale cluster]

    F6 --> F8{Metrics normal?}
    F8 --> F9[Monitor and observe]
    F8 --> F10[Optimize operator]
```

### ArgoCD Setup

#### Install ArgoCD Operator

```bash
# Install ArgoCD operator
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: argocd-operator
  namespace: argocd
spec:
  channel: stable
  name: argocd-operator
  source: community-operators
  sourceNamespace: openshift-marketplace
EOF

# Create ArgoCD instance
oc apply -f - <<EOF
apiVersion: argoproj.io/v1alpha1
kind: ArgoCD
metadata:
  name: argocd
  namespace: argocd
spec:
  server:
    route:
      enabled: true
  dex:
    image: quay.io/dexidp/dex
    version: v2.30.0
  rbac:
    defaultPolicy: 'role:readonly'
    policy: |
      g, system:cluster-admins, role:admin
EOF
```

### HashiCorp Vault with Secrets Operator

#### Install Vault Secrets Operator

```bash
# Install Vault Secrets Operator
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: vault-secrets-operator
  namespace: openshift-operators
spec:
  channel: stable
  name: vault-secrets-operator
  source: community-operators
  sourceNamespace: openshift-marketplace
EOF

# Create VaultConnection
oc apply -f - <<EOF
apiVersion: secrets.hashicorp.com/v1beta1
kind: VaultConnection
metadata:
  name: vault-connection
  namespace: openshift-operators
spec:
  address: https://vault.vault.svc:8200
  skipTLSVerify: false
  tlsServerName: vault.example.com
  caCertSecretRef: vault-ca-cert
EOF

# Create VaultAuth
oc apply -f - <<EOF
apiVersion: secrets.hashicorp.com/v1beta1
kind: VaultAuth
metadata:
  name: vault-auth
  namespace: openshift-operators
spec:
  method: kubernetes
  mountPath: /v1/auth/kubernetes
  kubernetes:
    mountPath: /v1/auth/kubernetes
    serviceAccount: vault-secrets-operator
    role: vault-secrets-operator
EOF
```

#### Configure Vault Instance

```yaml
# vault-instance.yaml
apiVersion: vault.banzaicloud.com/v1alpha1
kind: Vault
metadata:
  name: vault
  namespace: vault
spec:
  size: 3
  image: vault:1.12.2
  bankVaultsImage: banzaicloud/bank-vaults:1.15.1

  # TLS configuration
  tlsSecretName: vault-tls

  # External database for HA
  externalConfig:
    postgresql:
      host: vault-db.example.com
      port: 5432
      database: vault
      user: vault
      passwordSecret: vault-db-secret

  # Authentication methods
  auth:
    - type: kubernetes
    - type: oidc
      config:
        oidc_discovery_url: https://keycloak.example.com/auth/realms/master
        oidc_client_id: vault
        oidc_client_secret: vault-client-secret
```

#### Initialize Vault

```bash
# Initialize Vault (run on one pod)
oc exec -it vault-0 -n vault -- vault operator init

# Unseal Vault on all pods
for i in {0..2}; do
  oc exec -it vault-$i -n vault -- vault operator unseal
done

# Login and configure
oc exec -it vault-0 -n vault -- vault login

# Enable secrets engines
oc exec -it vault-0 -n vault -- vault secrets enable -path=secret kv-v2
oc exec -it vault-0 -n vault -- vault secrets enable pki
```

### Certificate Manager Configuration

#### Install cert-manager

```bash
# Install cert-manager operator
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: cert-manager
  namespace: openshift-operators
spec:
  channel: stable
  name: cert-manager
  source: community-operators
  sourceNamespace: openshift-marketplace
EOF
```

#### Configure Certificate Issuers

```yaml
# vault-issuer.yaml
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: vault-issuer
  namespace: cert-manager
spec:
  vault:
    server: https://vault.vault.svc:8200
    path: pki_int/sign/example-dot-com
    auth:
      kubernetes:
        mountPath: /v1/auth/kubernetes
        role: cert-manager
        serviceAccountRef:
          name: cert-manager
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: admin@example.com
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
      - http01:
          ingress:
            class: openshift-default
```

### OpenShift Operators Configuration

#### Install Core Operators

```bash
# Monitoring operator (pre-installed, but configure)
oc apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusOperator:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    prometheusK8s:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    alertmanagerMain:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    kubeStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    grafana:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    telemeterClient:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
EOF
```

#### Configure Logging Operator

```yaml
# cluster-logging.yaml
apiVersion: logging.openshift.io/v1
kind: ClusterLogging
metadata:
  name: instance
  namespace: openshift-logging
spec:
  managementState: Managed
  logStore:
    type: elasticsearch
    elasticsearch:
      nodeCount: 3
      storage:
        storageClassName: ocs-storagecluster-ceph-rbd
        size: 200G
      resources:
        requests:
          memory: 8Gi
          cpu: 2
  visualization:
    type: kibana
    kibana:
      replicas: 1
  collection:
    type: fluentd
```

### Security Configuration

#### Configure Security Context Constraints

```yaml
# custom-scc.yaml
apiVersion: security.openshift.io/v1
kind: SecurityContextConstraints
metadata:
  name: custom-scc
allowHostDirVolumePlugin: false
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivilegeEscalation: true
allowPrivilegedContainer: false
allowedCapabilities:
  - NET_BIND_SERVICE
defaultAddCapabilities: null
fsGroup:
  type: MustRunAs
groups:
  - system:authenticated
priority: 10
readOnlyRootFilesystem: false
requiredDropCapabilities:
  - KILL
  - MKNOD
  - SETUID
  - SETGID
runAsUser:
  type: MustRunAsRange
  uidRangeMin: 1000
  uidRangeMax: 2000
seLinuxContext:
  type: MustRunAs
supplementalGroups:
  type: RunAsAny
users: []
volumes:
  - configMap
  - downwardAPI
  - emptyDir
  - persistentVolumeClaim
  - projected
  - secret
```

#### Enable Compliance Operator

```bash
# Install compliance operator
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: compliance-operator
  namespace: openshift-compliance
spec:
  channel: stable
  name: compliance-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF

# Create compliance scan
oc apply -f - <<EOF
apiVersion: compliance.openshift.io/v1alpha1
kind: ComplianceScan
metadata:
  name: daily-compliance-scan
  namespace: openshift-compliance
spec:
  profile: xccdf_org.ssgproject.content_profile_moderate
  content: ssg-rhel8-ds.xml
  contentImage: quay.io/complianceascode/ocp4:latest
  rule: "xccdf_org.ssgproject.content_rule_no_direct_root_logins"
  scanType: Node
EOF
```

### Observability Stack Setup

#### Install Grafana, Loki, Mimir, and Alloy

```bash
# Install Grafana operator
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: grafana-operator
  namespace: openshift-operators
spec:
  channel: stable
  name: grafana-operator
  source: community-operators
  sourceNamespace: openshift-marketplace
EOF

# Install Loki operator
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: loki-operator
  namespace: openshift-operators
spec:
  channel: stable
  name: loki-operator
  source: community-operators
  sourceNamespace: openshift-marketplace
EOF

# Create LokiStack
oc apply -f - <<EOF
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: loki
  namespace: openshift-logging
spec:
  size: 1x.small
  storage:
    schemas:
    - version: v12
      effectiveDate: '2022-06-01'
    secret:
      name: loki-storage-secret
      type: s3
      endpoint: https://s3.example.com
      region: us-east-1
  storageClassName: trident-nas
  tenants:
    mode: openshift-logging
EOF

# Install Mimir operator
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: mimir-distributed
  namespace: openshift-operators
spec:
  channel: stable
  name: mimir-distributed
  source: community-operators
  sourceNamespace: openshift-marketplace
EOF

# Create Mimir instance
oc apply -f - <<EOF
apiVersion: grafana.integreatly.org/v1alpha1
kind: Mimir
metadata:
  name: mimir
  namespace: mimir
spec:
  replicas: 3
  storage:
    backend: s3
    s3:
      endpoint: https://s3.example.com
      region: us-east-1
      secretKey: mimir-storage-secret
  zones:
  - name: zone-a
  - name: zone-b
  - name: zone-c
EOF

# Install Alloy
oc apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: alloy-config
  namespace: alloy
data:
  config.alloy: |
    prometheus.scrape "default" {
      targets = [
        {"__address__" = "prometheus-k8s.openshift-monitoring.svc:9090"},
      ]
      forward_to = [prometheus.remote_write.default.receiver]
    }

    prometheus.remote_write "default" {
      endpoint {
        url = "http://mimir-gateway.mimir.svc:8080/api/v1/push"
      }
    }

    loki.write "default" {
      endpoint {
        url = "http://loki-gateway.openshift-logging.svc:8080/loki/api/v1/push"
      }
    }
EOF
```

### NetApp Trident Setup

#### Install Trident for Storage Orchestration

```bash
# Install Trident operator
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: trident-operator
  namespace: trident
spec:
  channel: stable
  name: trident-operator
  source: certified-operators
  sourceNamespace: openshift-marketplace
EOF

# Create Trident backend for ONTAP
oc apply -f - <<EOF
apiVersion: trident.netapp.io/v1
kind: TridentBackendConfig
metadata:
  name: ontap-nas-backend
  namespace: trident
spec:
  version: 1
  backendName: ontap-nas
  storageDriverName: ontap-nas
  svm: svm1
  managementLIF: 10.0.0.100
  dataLIF: 10.0.0.101
  username: vsadmin
  password: password
  storagePrefix: trident
EOF

# Create storage class for Trident
oc apply -f - <<EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: trident-nas
provisioner: csi.trident.netapp.io
parameters:
  backendType: ontap-nas
  fsType: nfs
reclaimPolicy: Delete
allowVolumeExpansion: true
EOF
```

### Ingress Controller Setup

#### Configure HAProxy and Nginx Ingress

```bash
# Install HAProxy ingress controller
oc apply -f - <<EOF
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: haproxy
  namespace: openshift-ingress-operator
spec:
  domain: apps.example.com
  replicas: 2
  endpointPublishingStrategy:
    type: LoadBalancerService
    loadBalancer:
      scope: External
  nodePlacement:
    nodeSelector:
      matchLabels:
        node-role.kubernetes.io/infra: ""
EOF

# Install Nginx ingress operator
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: nginx-ingress-operator
  namespace: openshift-operators
spec:
  channel: stable
  name: nginx-ingress-operator
  source: community-operators
  sourceNamespace: openshift-marketplace
EOF

# Create Nginx ingress controller
oc apply -f - <<EOF
apiVersion: k8s.nginx.org/v1
kind: NginxIngressController
metadata:
  name: nginx-ingress
  namespace: nginx-ingress
spec:
  type: deployment
  nginxPlus: false
  replicas: 2
  serviceType: LoadBalancer
  serviceAccount:
    name: nginx-ingress
EOF
```

### Daily Operations

#### Cluster Health Monitoring

```bash
# Check cluster status
oc get clusterversion
oc get clusteroperators

# Check node health
oc get nodes
oc describe nodes | grep -A 5 "Conditions:"

# Check pod health
oc get pods --all-namespaces | grep -v Running

# Check resource usage
oc adm top nodes
oc adm top pods --all-namespaces
```

#### Certificate Management

```bash
# Check certificate expiration
oc get certificates -A

# Renew certificates
oc renew certificate/tls-cert -n my-namespace

# Check Vault certificate status
vault list pki/certs
vault read pki/cert/<serial>
```

#### Backup Operations

```bash
# Backup etcd (management cluster)
oc get secrets -n openshift-etcd | grep etcd-all-certs
oc get secrets etcd-all-certs -n openshift-etcd -o yaml > etcd-certs-backup.yaml

# Backup application data
oc get pvc -A
velero backup create app-backup-$(date +%Y%m%d) --include-namespaces production

# Backup Vault data
vault operator raft snapshot save /tmp/vault-snapshot.snap
```

### Maintenance Procedures

#### Cluster Updates

```bash
# Check available updates
oc adm upgrade --to-latest=true

# Update cluster
oc adm upgrade --to=4.13.1

# Monitor update progress
oc get clusterversion -o yaml
oc logs -f deployment/machine-config-operator -n openshift-machine-config-operator
```

#### Node Maintenance

```bash
# Drain node for maintenance
oc adm drain node-1 --ignore-daemonsets --delete-emptydir-data

# Perform maintenance tasks
# ... maintenance work ...

# Uncordon node
oc adm uncordon node-1
```

#### Storage Maintenance

```bash
# Check storage health
oc get pv
oc describe pv/<pv-name>

# Resize PVC
oc patch pvc/my-pvc -p '{"spec":{"resources":{"requests":{"storage":"50Gi"}}}}'

# Check storage class
oc get storageclass
oc describe storageclass/ocs-storagecluster-ceph-rbd
```

### Scaling Operations

#### Horizontal Pod Autoscaling

```yaml
# hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
```

#### Cluster Scaling

```bash
# Scale worker nodes
oc scale machineset my-machineset -n openshift-machine-api --replicas=5

# Add new machineset for different instance type
oc apply -f new-machineset.yaml
```

### Disaster Recovery

#### Cluster Backup Strategy

```bash
# Create comprehensive backup
#!/bin/bash
BACKUP_DIR="/backups/$(date +%Y%m%d-%H%M%S)"
mkdir -p $BACKUP_DIR

# Backup etcd
oc get secrets -n openshift-etcd -o yaml > $BACKUP_DIR/etcd-secrets.yaml

# Backup cluster configuration
oc get configmaps -n openshift-config -o yaml > $BACKUP_DIR/configmaps.yaml
oc get secrets -n openshift-config -o yaml > $BACKUP_DIR/config-secrets.yaml

# Backup applications
velero backup create full-backup --include-namespaces production,staging

# Backup Vault
vault operator raft snapshot save $BACKUP_DIR/vault-snapshot.snap

echo "Backup completed: $BACKUP_DIR"
```

#### Recovery Procedures

```bash
# Restore from backup
#!/bin/bash
BACKUP_DIR="/backups/20231201-120000"

# Restore etcd
oc apply -f $BACKUP_DIR/etcd-secrets.yaml

# Restore cluster configuration
oc apply -f $BACKUP_DIR/configmaps.yaml
oc apply -f $BACKUP_DIR/config-secrets.yaml

# Restore applications
velero restore create --from-backup full-backup

# Restore Vault
vault operator raft snapshot restore $BACKUP_DIR/vault-snapshot.snap
```

### Common Issues and Solutions

#### Cluster Not Starting

**Symptoms:**

- Nodes not joining cluster
- API server unreachable
- etcd cluster unhealthy

**Troubleshooting Steps:**

```bash
# Check node status
oc get nodes
oc describe node <node-name>

# Check cluster operators
oc get co
oc describe co/<operator-name>

# Check etcd health
oc get pods -n openshift-etcd
oc logs etcd-<pod-name> -n openshift-etcd --previous

# Check certificates
oc get certificates -A | grep -i false
```

**Common Solutions:**

```bash
# Regenerate certificates
oc delete secrets -n openshift-etcd --selector template=etcd-peer,tls=true
oc delete secrets -n openshift-etcd --selector template=etcd-serving,tls=true

# Restart kubelet
oc debug node/<node-name> -- chroot /host systemctl restart kubelet

# Force delete stuck pods
oc delete pod <pod-name> --grace-period=0 --force
```

#### Application Deployment Issues

**Symptoms:**

- Pods stuck in Pending state
- Image pull errors
- Resource quota exceeded

**Troubleshooting:**

```bash
# Check pod status
oc describe pod/<pod-name>
oc get events --sort-by=.metadata.creationTimestamp

# Check resource quotas
oc get quota -A
oc describe quota <quota-name> -n <namespace>

# Check image pull secrets
oc get secrets -n <namespace> | grep docker
oc describe secret/<secret-name>

# Check network policies
oc get networkpolicies -n <namespace>
```

**Solutions:**

```yaml
# Fix resource requests/limits
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  template:
    spec:
      containers:
        - name: app
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"
```

#### Networking Issues

**Symptoms:**

- Service unreachable
- DNS resolution failures
- Pod-to-pod communication issues

**Troubleshooting:**

```bash
# Check service endpoints
oc get endpoints <service-name>
oc describe service <service-name>

# Test DNS resolution
oc run test-dns --image=busybox --rm -it -- nslookup <service-name>

# Check network policies
oc get networkpolicies
oc describe networkpolicy <policy-name>

# Check SDN status
oc get network.operator.openshift.io cluster -o yaml
```

**Solutions:**

```yaml
# Create network policy to allow traffic
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-internal
  namespace: production
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: production
  egress:
    - to:
        - namespaceSelector:
            matchLabels:
              name: production
```

#### Storage Issues

**Symptoms:**

- PVC stuck in Pending
- Pod cannot mount volumes
- Storage full errors

**Troubleshooting:**

```bash
# Check PVC status
oc describe pvc <pvc-name>
oc get events --field-selector involvedObject.name=<pvc-name>

# Check storage class
oc get storageclass
oc describe storageclass <storage-class-name>

# Check PV status
oc get pv
oc describe pv <pv-name>

# Check storage cluster health (if using OCS)
oc get storagecluster -n openshift-storage
```

**Solutions:**

```bash
# Create storage class
oc apply -f - <<EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  fsType: ext4
reclaimPolicy: Delete
EOF

# Resize PVC (if supported)
oc patch pvc/<pvc-name> -p '{"spec":{"resources":{"requests":{"storage":"100Gi"}}}}'
```

#### Performance Issues

**Symptoms:**

- High latency
- Resource exhaustion
- Slow application response

**Troubleshooting:**

```bash
# Check resource usage
oc adm top nodes
oc adm top pods

# Check cluster autoscaler
oc get clusterautoscaler
oc describe clusterautoscaler default

# Check HPA
oc get hpa
oc describe hpa <hpa-name>

# Check monitoring alerts
oc get alerts -n openshift-monitoring
```

**Solutions:**

```yaml
# Configure HPA
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web
  minReplicas: 3
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
```

### Log Analysis

#### Collecting Logs

```bash
# Collect cluster logs
oc adm must-gather --dest-dir=/tmp/must-gather

# Collect specific pod logs
oc logs <pod-name> -n <namespace> --previous

# Collect node logs
oc adm node-logs <node-name> --path=kubelet

# Collect operator logs
oc logs deployment/<operator-name> -n <operator-namespace>
```

#### Log Analysis Commands

```bash
# Search for errors in logs
oc logs <pod-name> | grep -i error

# Check audit logs
oc adm node-logs <node-name> --path=audit/audit.log | jq '.'

# Analyze resource usage patterns
oc adm top pods --all-namespaces --sort-by=cpu
```

### Getting Help

#### Support Resources

1. **Red Hat Support Portal**: https://access.redhat.com/support
2. **OpenShift Documentation**: https://docs.openshift.com/
3. **Community Forums**: https://www.redhat.com/en/blog/red-hat-openShift-commons
4. **Knowledge Base**: https://access.redhat.com/articles/

#### Escalation Procedures

```bash
# Generate support case data
oc adm must-gather --dest-dir=/tmp/support-case

# Check cluster ID for support
oc get clusterversion -o jsonpath='{.items[0].spec.clusterID}'

# Collect sosreport from nodes
oc debug node/<node-name> -- chroot /host sosreport -o openshift
```

### API Reference

#### OpenShift APIs

| Resource          | API Version               | Description                       |
| ----------------- | ------------------------- | --------------------------------- |
| `clusterversion`  | `config.openshift.io/v1`  | Cluster version and update status |
| `clusteroperator` | `config.openshift.io/v1`  | Cluster operator status           |
| `infrastructure`  | `config.openshift.io/v1`  | Infrastructure configuration      |
| `ingress`         | `config.openshift.io/v1`  | Ingress configuration             |
| `network`         | `config.openshift.io/v1`  | Network configuration             |
| `oauth`           | `config.openshift.io/v1`  | OAuth configuration               |
| `project`         | `project.openshift.io/v1` | Project management                |
| `route`           | `route.openshift.io/v1`   | Route configuration               |

#### Key Commands

```bash
# Cluster information
oc version                                    # OpenShift version
oc get clusterversion                         # Cluster version status
oc get clusteroperators                       # Operator status
oc get nodes                                  # Node information
oc get projects                               # Project list

# Resource management
oc get pods -A                                # All pods
oc get deployments -A                         # All deployments
oc get services -A                            # All services
oc get routes -A                              # All routes
oc get pvc -A                                 # Persistent volume claims

# Troubleshooting
oc describe pod/<name>                        # Pod details
oc logs pod/<name>                            # Pod logs
oc get events --sort-by=.metadata.creationTimestamp  # Events
oc adm top nodes                              # Node resource usage
oc adm top pods                               # Pod resource usage

# Configuration
oc get configmaps -A                          # ConfigMaps
oc get secrets -A                             # Secrets
oc get networkpolicies -A                     # Network policies
oc get scc                                    # Security context constraints
```

### Configuration Examples

#### Sample Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: production
  labels:
    app: web-app
    version: v1.0.0
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
        version: v1.0.0
    spec:
      containers:
        - name: web
          image: nginx:1.21
          ports:
            - containerPort: 80
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"
          livenessProbe:
            httpGet:
              path: /health
              port: 80
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: 80
            initialDelaySeconds: 5
            periodSeconds: 5
      securityContext:
        runAsNonRoot: true
        runAsUser: 101
      serviceAccountName: web-app-sa
```

#### Sample Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-app
  namespace: production
  labels:
    app: web-app
spec:
  selector:
    app: web-app
  ports:
    - name: http
      port: 80
      targetPort: 80
      protocol: TCP
  type: ClusterIP
```

#### Sample Route

```yaml
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: web-app
  namespace: production
  labels:
    app: web-app
spec:
  host: web-app.example.com
  to:
    kind: Service
    name: web-app
  port:
    targetPort: http
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None
```

### Security Best Practices

#### Pod Security

- Use `runAsNonRoot: true` in security contexts
- Set appropriate `runAsUser` and `runAsGroup`
- Use `readOnlyRootFilesystem: true` when possible
- Define resource requests and limits
- Use security context constraints appropriately

#### Network Security

- Implement network policies to restrict traffic
- Use TLS for all external communications
- Implement proper ingress controls
- Use network policies for microservices security

#### Secret Management

- Never store secrets in code or config files
- Use HashiCorp Vault for secret storage
- Rotate secrets regularly
- Implement proper RBAC for secret access
- Use sealed secrets for GitOps workflows

### Performance Tuning

#### Cluster Sizing Guidelines

| Component                       | CPU Cores | Memory (GB) | Storage (GB) |
| ------------------------------- | --------- | ----------- | ------------ |
| Control Plane Node              | 8-16      | 32-64       | 100          |
| Worker Node (General)           | 4-8       | 16-32       | 50           |
| Worker Node (Compute Intensive) | 16-32     | 64-128      | 100          |
| Infrastructure Node             | 4         | 16          | 50           |

#### Monitoring Thresholds

- CPU utilization: Warning > 80%, Critical > 90%
- Memory utilization: Warning > 85%, Critical > 95%
- Disk utilization: Warning > 75%, Critical > 85%
- Pod restarts: Warning > 5/hour, Critical > 10/hour

### Glossary

| Term                | Definition                                               |
| ------------------- | -------------------------------------------------------- |
| **ClusterOperator** | OpenShift component that manages cluster functionality   |
| **Control Plane**   | Master nodes that manage the cluster                     |
| **CRD**             | Custom Resource Definition - extends Kubernetes API      |
| **etcd**            | Distributed key-value store for cluster state            |
| **HPA**             | Horizontal Pod Autoscaler - scales pods based on metrics |
| **HyperShift**      | Technology for hosting control planes as pods            |
| **MachineSet**      | Template for creating machines (VMs/instances)           |
| **Namespace**       | Virtual cluster within a physical cluster                |
| **Node**            | Worker machine in the cluster                            |
| **Operator**        | Kubernetes-native application manager                    |
| **Pod**             | Smallest deployable unit in Kubernetes                   |
| **PV**              | Persistent Volume - storage resource                     |
| **PVC**             | Persistent Volume Claim - request for storage            |
| **SCC**             | Security Context Constraints - pod security policies     |
| **Service**         | Abstraction for accessing pods                           |
| **ServiceAccount**  | Account for processes running in pods                    |

### Additional Resources

- **OpenShift Documentation**: https://docs.openshift.com/container-platform/
- **Kubernetes Documentation**: https://kubernetes.io/docs/
- **Red Hat Customer Portal**: https://access.redhat.com/
- **OpenShift Blog**: https://www.redhat.com/en/blog/channel/red-hat-openshift
- **Community Forums**: https://www.redhat.com/en/blog/red-hat-openShift-commons

---

_This documentation is maintained by the Platform Engineering team. For contributions or updates, please submit a pull request to the documentation repository._
